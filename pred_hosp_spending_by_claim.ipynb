{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 204,
   "source": [
    "import pandas as pd\r\n",
    "import numpy as np\r\n",
    "import warnings\r\n",
    "import matplotlib.pyplot as plt\r\n",
    "import pyodbc\r\n",
    "import pandas_access as pa\r\n",
    "from sklearn.model_selection import train_test_split\r\n",
    "from sklearn.linear_model import LogisticRegression\r\n",
    "from sklearn.svm import SVC, LinearSVC\r\n",
    "from sklearn.ensemble import RandomForestClassifier\r\n",
    "from sklearn.neighbors import KNeighborsClassifier\r\n",
    "from sklearn.naive_bayes import GaussianNB\r\n",
    "from sklearn.linear_model import Perceptron\r\n",
    "from sklearn.linear_model import SGDClassifier\r\n",
    "from sklearn.tree import DecisionTreeClassifier\r\n",
    "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix, roc_auc_score,classification_report\r\n",
    "from sklearn.model_selection import cross_val_predict,GridSearchCV, StratifiedKFold\r\n",
    "from sklearn.ensemble import IsolationForest\r\n",
    "from sklearn.svm import OneClassSVM\r\n",
    "from sklearn.covariance import EllipticEnvelope\r\n",
    "from xgboost import XGBClassifier\r\n",
    "from sklearn.preprocessing import StandardScaler,MinMaxScaler  \r\n",
    "from sklearn.neural_network import MLPClassifier\r\n",
    "from tensorflow import keras\r\n",
    "import tensorflow as tf\r\n",
    "import imblearn\r\n",
    "from imblearn.over_sampling import SMOTE,BorderlineSMOTE,ADASYN,SVMSMOTE\r\n",
    "from imblearn.under_sampling import RandomUnderSampler\r\n",
    "from imblearn.pipeline import Pipeline as imbpipeline\r\n",
    "import seaborn as sns\r\n",
    "from collections import Counter\r\n",
    "import loras"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "source": [
    "pd.set_option('display.max_columns',200)\r\n",
    "pd.set_option('display.max_rows',200)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "source": [
    "dataset_close=pd.read_csv('data\\hospital_closure\\hospital_closure.csv')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "source": [
    "list(cur_2017.tables())"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[('data\\\\hospital_compare\\\\hc_apr2017\\\\Hospital.mdb', None, 'MSysAccessObjects', 'SYSTEM TABLE', None),\n",
       " ('data\\\\hospital_compare\\\\hc_apr2017\\\\Hospital.mdb', None, 'MSysAccessXML', 'SYSTEM TABLE', None),\n",
       " ('data\\\\hospital_compare\\\\hc_apr2017\\\\Hospital.mdb', None, 'MSysACEs', 'SYSTEM TABLE', None),\n",
       " ('data\\\\hospital_compare\\\\hc_apr2017\\\\Hospital.mdb', None, 'MSysIMEXColumns', 'SYSTEM TABLE', None),\n",
       " ('data\\\\hospital_compare\\\\hc_apr2017\\\\Hospital.mdb', None, 'MSysIMEXSpecs', 'SYSTEM TABLE', None),\n",
       " ('data\\\\hospital_compare\\\\hc_apr2017\\\\Hospital.mdb', None, 'MSysNameMap', 'SYSTEM TABLE', None),\n",
       " ('data\\\\hospital_compare\\\\hc_apr2017\\\\Hospital.mdb', None, 'MSysNavPaneGroupCategories', 'SYSTEM TABLE', None),\n",
       " ('data\\\\hospital_compare\\\\hc_apr2017\\\\Hospital.mdb', None, 'MSysNavPaneGroups', 'SYSTEM TABLE', None),\n",
       " ('data\\\\hospital_compare\\\\hc_apr2017\\\\Hospital.mdb', None, 'MSysNavPaneGroupToObjects', 'SYSTEM TABLE', None),\n",
       " ('data\\\\hospital_compare\\\\hc_apr2017\\\\Hospital.mdb', None, 'MSysNavPaneObjectIDs', 'SYSTEM TABLE', None),\n",
       " ('data\\\\hospital_compare\\\\hc_apr2017\\\\Hospital.mdb', None, 'MSysObjects', 'SYSTEM TABLE', None),\n",
       " ('data\\\\hospital_compare\\\\hc_apr2017\\\\Hospital.mdb', None, 'MSysQueries', 'SYSTEM TABLE', None),\n",
       " ('data\\\\hospital_compare\\\\hc_apr2017\\\\Hospital.mdb', None, 'MSysRelationships', 'SYSTEM TABLE', None),\n",
       " ('data\\\\hospital_compare\\\\hc_apr2017\\\\Hospital.mdb', None, 'Ambulatory Surgical Measures-Facility', 'TABLE', None),\n",
       " ('data\\\\hospital_compare\\\\hc_apr2017\\\\Hospital.mdb', None, 'Ambulatory Surgical Measures-National', 'TABLE', None),\n",
       " ('data\\\\hospital_compare\\\\hc_apr2017\\\\Hospital.mdb', None, 'Ambulatory Surgical Measures-State', 'TABLE', None),\n",
       " ('data\\\\hospital_compare\\\\hc_apr2017\\\\Hospital.mdb', None, 'FY2015_Distribution_of_Net_Change_in_Base_Op_DRG_Payment_Amt', 'TABLE', None),\n",
       " ('data\\\\hospital_compare\\\\hc_apr2017\\\\Hospital.mdb', None, 'FY2015_Net_Change_in_Base_Op_DRG_Payment_Amt', 'TABLE', None),\n",
       " ('data\\\\hospital_compare\\\\hc_apr2017\\\\Hospital.mdb', None, 'FY2015_Percent_Change_in_Medicare_Payments', 'TABLE', None),\n",
       " ('data\\\\hospital_compare\\\\hc_apr2017\\\\Hospital.mdb', None, 'FY2015_Value_Based_Incentive_Payment_Amount', 'TABLE', None),\n",
       " ('data\\\\hospital_compare\\\\hc_apr2017\\\\Hospital.mdb', None, 'GLOBAL_April2017_09March2017', 'TABLE', None),\n",
       " ('data\\\\hospital_compare\\\\hc_apr2017\\\\Hospital.mdb', None, 'HOSPITAL_QUARTERLY_HAC_DOMAIN_HOSPITAL', 'TABLE', None),\n",
       " ('data\\\\hospital_compare\\\\hc_apr2017\\\\Hospital.mdb', None, 'HOSPITAL_QUARTERLY_IPFQR_MEASURES_HOSPITAL', 'TABLE', None),\n",
       " ('data\\\\hospital_compare\\\\hc_apr2017\\\\Hospital.mdb', None, 'HOSPITAL_QUARTERLY_IPFQR_MEASURES_NATIONAL', 'TABLE', None),\n",
       " ('data\\\\hospital_compare\\\\hc_apr2017\\\\Hospital.mdb', None, 'HOSPITAL_QUARTERLY_IPFQR_MEASURES_STATE', 'TABLE', None),\n",
       " ('data\\\\hospital_compare\\\\hc_apr2017\\\\Hospital.mdb', None, 'HOSPITAL_QUARTERLY_MSPB_6_DECIMALS', 'TABLE', None),\n",
       " ('data\\\\hospital_compare\\\\hc_apr2017\\\\Hospital.mdb', None, 'HOSPITAL_QUARTERLY_QUALITYMEASURE_PCH_HCAHPS_HOSPITAL', 'TABLE', None),\n",
       " ('data\\\\hospital_compare\\\\hc_apr2017\\\\Hospital.mdb', None, 'HOSPITAL_QUARTERLY_QUALITYMEASURE_PCH_HCAHPS_NATIONAL', 'TABLE', None),\n",
       " ('data\\\\hospital_compare\\\\hc_apr2017\\\\Hospital.mdb', None, 'HOSPITAL_QUARTERLY_QUALITYMEASURE_PCH_HCAHPS_STATE', 'TABLE', None),\n",
       " ('data\\\\hospital_compare\\\\hc_apr2017\\\\Hospital.mdb', None, 'HOSPITAL_QUARTERLY_QUALITYMEASURE_PCH_HOSPITAL', 'TABLE', None),\n",
       " ('data\\\\hospital_compare\\\\hc_apr2017\\\\Hospital.mdb', None, 'HOSPITAL_QUARTERLY_QUALITYMEASURE_PCH_OCM_HOSPITAL', 'TABLE', None),\n",
       " ('data\\\\hospital_compare\\\\hc_apr2017\\\\Hospital.mdb', None, 'HQI_FTNT', 'TABLE', None),\n",
       " ('data\\\\hospital_compare\\\\hc_apr2017\\\\Hospital.mdb', None, 'HQI_HOSP', 'TABLE', None),\n",
       " ('data\\\\hospital_compare\\\\hc_apr2017\\\\Hospital.mdb', None, 'HQI_HOSP_Comp', 'TABLE', None),\n",
       " ('data\\\\hospital_compare\\\\hc_apr2017\\\\Hospital.mdb', None, 'HQI_HOSP_HAI', 'TABLE', None),\n",
       " ('data\\\\hospital_compare\\\\hc_apr2017\\\\Hospital.mdb', None, 'HQI_HOSP_HCAHPS', 'TABLE', None),\n",
       " ('data\\\\hospital_compare\\\\hc_apr2017\\\\Hospital.mdb', None, 'HQI_HOSP_IMG', 'TABLE', None),\n",
       " ('data\\\\hospital_compare\\\\hc_apr2017\\\\Hospital.mdb', None, 'HQI_HOSP_MSPB', 'TABLE', None),\n",
       " ('data\\\\hospital_compare\\\\hc_apr2017\\\\Hospital.mdb', None, 'HQI_HOSP_PaymentAndValueOfCare', 'TABLE', None),\n",
       " ('data\\\\hospital_compare\\\\hc_apr2017\\\\Hospital.mdb', None, 'HQI_HOSP_ReadmDeath', 'TABLE', None),\n",
       " ('data\\\\hospital_compare\\\\hc_apr2017\\\\Hospital.mdb', None, 'HQI_HOSP_STRUCTURAL', 'TABLE', None),\n",
       " ('data\\\\hospital_compare\\\\hc_apr2017\\\\Hospital.mdb', None, 'HQI_HOSP_TimelyEffectiveCare', 'TABLE', None),\n",
       " ('data\\\\hospital_compare\\\\hc_apr2017\\\\Hospital.mdb', None, 'HQI_NATIONAL_Comp', 'TABLE', None),\n",
       " ('data\\\\hospital_compare\\\\hc_apr2017\\\\Hospital.mdb', None, 'HQI_NATIONAL_HAI', 'TABLE', None),\n",
       " ('data\\\\hospital_compare\\\\hc_apr2017\\\\Hospital.mdb', None, 'HQI_NATIONAL_HCAHPS', 'TABLE', None),\n",
       " ('data\\\\hospital_compare\\\\hc_apr2017\\\\Hospital.mdb', None, 'HQI_NATIONAL_IMG_AVG', 'TABLE', None),\n",
       " ('data\\\\hospital_compare\\\\hc_apr2017\\\\Hospital.mdb', None, 'HQI_NATIONAL_MSPB', 'TABLE', None),\n",
       " ('data\\\\hospital_compare\\\\hc_apr2017\\\\Hospital.mdb', None, 'HQI_NATIONAL_Payment', 'TABLE', None),\n",
       " ('data\\\\hospital_compare\\\\hc_apr2017\\\\Hospital.mdb', None, 'HQI_NATIONAL_ReadmDeath', 'TABLE', None),\n",
       " ('data\\\\hospital_compare\\\\hc_apr2017\\\\Hospital.mdb', None, 'HQI_NATIONAL_TimelyEffectiveCare', 'TABLE', None),\n",
       " ('data\\\\hospital_compare\\\\hc_apr2017\\\\Hospital.mdb', None, 'HQI_NATIONAL_Value of Care', 'TABLE', None),\n",
       " ('data\\\\hospital_compare\\\\hc_apr2017\\\\Hospital.mdb', None, 'HQI_STATE_Comp', 'TABLE', None),\n",
       " ('data\\\\hospital_compare\\\\hc_apr2017\\\\Hospital.mdb', None, 'HQI_STATE_HAI', 'TABLE', None),\n",
       " ('data\\\\hospital_compare\\\\hc_apr2017\\\\Hospital.mdb', None, 'HQI_STATE_HCAHPS', 'TABLE', None),\n",
       " ('data\\\\hospital_compare\\\\hc_apr2017\\\\Hospital.mdb', None, 'HQI_STATE_IMG_AVG', 'TABLE', None),\n",
       " ('data\\\\hospital_compare\\\\hc_apr2017\\\\Hospital.mdb', None, 'HQI_STATE_MSPB', 'TABLE', None),\n",
       " ('data\\\\hospital_compare\\\\hc_apr2017\\\\Hospital.mdb', None, 'HQI_STATE_Payment', 'TABLE', None),\n",
       " ('data\\\\hospital_compare\\\\hc_apr2017\\\\Hospital.mdb', None, 'HQI_STATE_ReadmDeath', 'TABLE', None),\n",
       " ('data\\\\hospital_compare\\\\hc_apr2017\\\\Hospital.mdb', None, 'HQI_STATE_TimelyEffectiveCare', 'TABLE', None),\n",
       " ('data\\\\hospital_compare\\\\hc_apr2017\\\\Hospital.mdb', None, 'Hvbp_ami_11_14_2016', 'TABLE', None),\n",
       " ('data\\\\hospital_compare\\\\hc_apr2017\\\\Hospital.mdb', None, 'Hvbp_clinical_care_outcomes_11_10_2016', 'TABLE', None),\n",
       " ('data\\\\hospital_compare\\\\hc_apr2017\\\\Hospital.mdb', None, 'Hvbp_efficiency_11_10_2016', 'TABLE', None),\n",
       " ('data\\\\hospital_compare\\\\hc_apr2017\\\\Hospital.mdb', None, 'Hvbp_hcahps_11_10_2016', 'TABLE', None),\n",
       " ('data\\\\hospital_compare\\\\hc_apr2017\\\\Hospital.mdb', None, 'Hvbp_imm2_11_10_2016', 'TABLE', None),\n",
       " ('data\\\\hospital_compare\\\\hc_apr2017\\\\Hospital.mdb', None, 'Hvbp_pc_11_10_2016', 'TABLE', None),\n",
       " ('data\\\\hospital_compare\\\\hc_apr2017\\\\Hospital.mdb', None, 'Hvbp_safety_11_10_2016', 'TABLE', None),\n",
       " ('data\\\\hospital_compare\\\\hc_apr2017\\\\Hospital.mdb', None, 'Hvbp_tps_11_10_2016', 'TABLE', None),\n",
       " ('data\\\\hospital_compare\\\\hc_apr2017\\\\Hospital.mdb', None, 'Measure_Dates', 'TABLE', None),\n",
       " ('data\\\\hospital_compare\\\\hc_apr2017\\\\Hospital.mdb', None, 'Medicare Hospital Spending by Claim', 'TABLE', None),\n",
       " ('data\\\\hospital_compare\\\\hc_apr2017\\\\Hospital.mdb', None, 'MORT_READM_April2017', 'TABLE', None),\n",
       " ('data\\\\hospital_compare\\\\hc_apr2017\\\\Hospital.mdb', None, 'Outpatient Procedures - Volume', 'TABLE', None),\n",
       " ('data\\\\hospital_compare\\\\hc_apr2017\\\\Hospital.mdb', None, 'PSI_April2017', 'TABLE', None),\n",
       " ('data\\\\hospital_compare\\\\hc_apr2017\\\\Hospital.mdb', None, 'VA_HBIPS_December2016_CMS_Submission', 'TABLE', None),\n",
       " ('data\\\\hospital_compare\\\\hc_apr2017\\\\Hospital.mdb', None, 'VA_IPSHEP_Apr2017CMS_09MAR17', 'TABLE', None),\n",
       " ('data\\\\hospital_compare\\\\hc_apr2017\\\\Hospital.mdb', None, 'vwHQI_READM_REDUCTION', 'TABLE', None),\n",
       " ('data\\\\hospital_compare\\\\hc_apr2017\\\\Hospital.mdb', None, 'HQI_HOSP_MSPB Query', 'VIEW', None)]"
      ]
     },
     "metadata": {},
     "execution_count": 99
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "source": [
    "MDB_2017='data\\hospital_compare\\hc_apr2017\\Hospital.mdb'\r\n",
    "MDB_2016 = 'data\\hospital_compare\\hc_may2016\\Hospital.mdb'\r\n",
    "MDB_2015='data\\hospital_compare\\Copy of HospitalMay2015.mdb'\r\n",
    "MDB_2014='data\\hospital_compare\\HC_May2014.mdb'\r\n",
    "\r\n",
    "DRV = '{Microsoft Access Driver (*.mdb, *.accdb)}'\r\n",
    "PWD = 'pw'\r\n",
    "\r\n",
    "con_2017 = pyodbc.connect('DRIVER={};DBQ={};PWD={}'.format(DRV,MDB_2017,PWD))\r\n",
    "con_2016 = pyodbc.connect('DRIVER={};DBQ={};PWD={}'.format(DRV,MDB_2016,PWD))\r\n",
    "con_2015 = pyodbc.connect('DRIVER={};DBQ={};PWD={}'.format(DRV,MDB_2015,PWD))\r\n",
    "con_2014 = pyodbc.connect('DRIVER={};DBQ={};PWD={}'.format(DRV,MDB_2014,PWD))\r\n",
    "cur_2017 = con_2017.cursor()\r\n",
    "cur_2016 = con_2016.cursor()\r\n",
    "cur_2015 = con_2015.cursor()\r\n",
    "cur_2014 = con_2014.cursor()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "source": [
    "table_name = '[Medicare Hospital Spending by Claim]'\r\n",
    "\r\n",
    "query = \"SELECT * FROM {}\".format(table_name)\r\n",
    "\r\n",
    "query3 = \"SELECT * FROM {}.COLUMNS\".format(table_name)\r\n",
    "rows_2017 = cur_2017.execute(query).fetchall()\r\n",
    "rows_2016 = cur_2016.execute(query).fetchall()\r\n",
    "rows_2015 = cur_2015.execute(query).fetchall()\r\n",
    "rows_2014 = cur_2014.execute(query).fetchall()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "cur_2016.execute(query).fetchone()"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "('SOUTHEAST ALABAMA MEDICAL CENTER', '010001', 'AL', '1 to 3 days Prior to Index Hospital Admission', 'Home Health Agency', '$12', '$14', '$13', '0.06%', '0.07%', '0.07%', '01/01/2014', '12/31/2014')"
      ]
     },
     "metadata": {},
     "execution_count": 72
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "for row in cur_2017.columns(table='Medicare Hospital Spending by Claim'):\r\n",
    "    print(row.column_name)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Hospital_Name\n",
      "Provider_ID\n",
      "State\n",
      "Period\n",
      "Claim_Type\n",
      "Avg_Spending_Per_Episode_Hospital\n",
      "Avg_Spending_Per_Episode_State\n",
      "Avg_Spending_Per_Episode_Nation\n",
      "Percent_of_Spending_Hospital\n",
      "Percent_of_Spending_State\n",
      "Percent_of_Spending_Nation\n",
      "Start_Date\n",
      "End_Date\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "source": [
    "arr_2017=np.array(rows_2017)\r\n",
    "dataset_2017=pd.DataFrame(arr_2017[:,[1,5,6,7,8,9,10]],columns=['id','1','2','3','4','5','6'])\r\n",
    "arr_2016=np.array(rows_2016)\r\n",
    "dataset_2016=pd.DataFrame(arr_2016[:,[1,5,6,7,8,9,10]],columns=['id','1','2','3','4','5','6'])\r\n",
    "arr_2015=np.array(rows_2015)\r\n",
    "dataset_2015=pd.DataFrame(arr_2015[:,[1,5,6,7,8,9,10]],columns=['id','1','2','3','4','5','6'])\r\n",
    "arr_2014=np.array(rows_2014)\r\n",
    "dataset_2014=pd.DataFrame(arr_2014[:,[1,5,6,7,8,9,10]],columns=['id','1','2','3','4','5','6'])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "source": [
    "dataset_2014"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "            id         1         2         3        4        5        6\n",
       "0       010001       $11       $14       $13    0.06%    0.08%    0.07%\n",
       "1       010001        $4        $2        $1    0.02%    0.01%    0.00%\n",
       "2       010001        $9        $5        $5    0.05%    0.03%    0.03%\n",
       "3       010001       $71       $51       $63    0.38%    0.29%    0.34%\n",
       "4       010001        $1        $2        $2    0.01%    0.01%    0.01%\n",
       "...        ...       ...       ...       ...      ...      ...      ...\n",
       "143435  670082     $537      $647      $624     2.92%    3.25%    3.33%\n",
       "143436  670082   $2,682    $2,516    $2,924    14.56%   12.65%   15.63%\n",
       "143437  670082     $135      $132      $112     0.73%    0.66%    0.60%\n",
       "143438  670082   $1,228    $1,124    $1,005     6.67%    5.65%    5.37%\n",
       "143439  670082  $18,413   $19,886   $18,704   100.00%  100.00%  100.00%\n",
       "\n",
       "[143440 rows x 7 columns]"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>010001</td>\n",
       "      <td>$11</td>\n",
       "      <td>$14</td>\n",
       "      <td>$13</td>\n",
       "      <td>0.06%</td>\n",
       "      <td>0.08%</td>\n",
       "      <td>0.07%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>010001</td>\n",
       "      <td>$4</td>\n",
       "      <td>$2</td>\n",
       "      <td>$1</td>\n",
       "      <td>0.02%</td>\n",
       "      <td>0.01%</td>\n",
       "      <td>0.00%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>010001</td>\n",
       "      <td>$9</td>\n",
       "      <td>$5</td>\n",
       "      <td>$5</td>\n",
       "      <td>0.05%</td>\n",
       "      <td>0.03%</td>\n",
       "      <td>0.03%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>010001</td>\n",
       "      <td>$71</td>\n",
       "      <td>$51</td>\n",
       "      <td>$63</td>\n",
       "      <td>0.38%</td>\n",
       "      <td>0.29%</td>\n",
       "      <td>0.34%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>010001</td>\n",
       "      <td>$1</td>\n",
       "      <td>$2</td>\n",
       "      <td>$2</td>\n",
       "      <td>0.01%</td>\n",
       "      <td>0.01%</td>\n",
       "      <td>0.01%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>143435</th>\n",
       "      <td>670082</td>\n",
       "      <td>$537</td>\n",
       "      <td>$647</td>\n",
       "      <td>$624</td>\n",
       "      <td>2.92%</td>\n",
       "      <td>3.25%</td>\n",
       "      <td>3.33%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>143436</th>\n",
       "      <td>670082</td>\n",
       "      <td>$2,682</td>\n",
       "      <td>$2,516</td>\n",
       "      <td>$2,924</td>\n",
       "      <td>14.56%</td>\n",
       "      <td>12.65%</td>\n",
       "      <td>15.63%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>143437</th>\n",
       "      <td>670082</td>\n",
       "      <td>$135</td>\n",
       "      <td>$132</td>\n",
       "      <td>$112</td>\n",
       "      <td>0.73%</td>\n",
       "      <td>0.66%</td>\n",
       "      <td>0.60%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>143438</th>\n",
       "      <td>670082</td>\n",
       "      <td>$1,228</td>\n",
       "      <td>$1,124</td>\n",
       "      <td>$1,005</td>\n",
       "      <td>6.67%</td>\n",
       "      <td>5.65%</td>\n",
       "      <td>5.37%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>143439</th>\n",
       "      <td>670082</td>\n",
       "      <td>$18,413</td>\n",
       "      <td>$19,886</td>\n",
       "      <td>$18,704</td>\n",
       "      <td>100.00%</td>\n",
       "      <td>100.00%</td>\n",
       "      <td>100.00%</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>143440 rows × 7 columns</p>\n",
       "</div>"
      ]
     },
     "metadata": {},
     "execution_count": 105
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "source": [
    "dataset_2017['1']=dataset_2017['1'].str.strip(\"$\")\r\n",
    "dataset_2017['1']=dataset_2017['1'].str.replace(',', '')\r\n",
    "dataset_2017['2']=dataset_2017['2'].str.strip(\"$\")\r\n",
    "dataset_2017['2']=dataset_2017['2'].str.replace(',', '')\r\n",
    "dataset_2017['3']=dataset_2017['3'].str.strip(\"$\")\r\n",
    "dataset_2017['3']=dataset_2017['3'].str.replace(',', '')\r\n",
    "dataset_2016['1']=dataset_2016['1'].str.strip(\"$\")\r\n",
    "dataset_2016['1']=dataset_2016['1'].str.replace(',', '')\r\n",
    "dataset_2016['2']=dataset_2016['2'].str.strip(\"$\")\r\n",
    "dataset_2016['2']=dataset_2016['2'].str.replace(',', '')\r\n",
    "dataset_2016['3']=dataset_2016['3'].str.strip(\"$\")\r\n",
    "dataset_2016['3']=dataset_2016['3'].str.replace(',', '')\r\n",
    "dataset_2015['1']=dataset_2015['1'].str.strip(\"$\")\r\n",
    "dataset_2015['1']=dataset_2015['1'].str.replace(',', '')\r\n",
    "dataset_2015['2']=dataset_2015['2'].str.strip(\"$\")\r\n",
    "dataset_2015['2']=dataset_2015['2'].str.replace(',', '')\r\n",
    "dataset_2015['3']=dataset_2015['3'].str.strip(\"$\")\r\n",
    "dataset_2015['3']=dataset_2015['3'].str.replace(',', '')\r\n",
    "dataset_2014['1']=dataset_2014['1'].str.strip(\"$\")\r\n",
    "dataset_2014['1']=dataset_2014['1'].str.replace(',', '')\r\n",
    "dataset_2014['2']=dataset_2014['2'].str.strip(\"$\")\r\n",
    "dataset_2014['2']=dataset_2014['2'].str.replace(',', '')\r\n",
    "dataset_2014['3']=dataset_2014['3'].str.strip(\"$\")\r\n",
    "dataset_2014['3']=dataset_2014['3'].str.replace(',', '')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "source": [
    "dataset_2017['4']=dataset_2017['4'].str.strip(\"%\").astype(float)/100\r\n",
    "dataset_2017['5']=dataset_2017['5'].str.strip(\"%\").astype(float)/100\r\n",
    "dataset_2017['6']=dataset_2017['6'].str.strip(\"%\").astype(float)/100\r\n",
    "dataset_2016['4']=dataset_2016['4'].str.strip(\"%\").astype(float)/100\r\n",
    "dataset_2016['5']=dataset_2016['5'].str.strip(\"%\").astype(float)/100\r\n",
    "dataset_2016['6']=dataset_2016['6'].str.strip(\"%\").astype(float)/100\r\n",
    "dataset_2015['4']=dataset_2015['4'].str.strip(\"%\").astype(float)/100\r\n",
    "dataset_2015['5']=dataset_2015['5'].str.strip(\"%\").astype(float)/100\r\n",
    "dataset_2015['6']=dataset_2015['6'].str.strip(\"%\").astype(float)/100\r\n",
    "dataset_2014['4']=dataset_2014['4'].str.strip(\"%\").astype(float)/100\r\n",
    "dataset_2014['5']=dataset_2014['5'].str.strip(\"%\").astype(float)/100\r\n",
    "dataset_2014['6']=dataset_2014['6'].str.strip(\"%\").astype(float)/100"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "source": [
    "for col in dataset_2017:\r\n",
    "    dataset_2017[col]=pd.to_numeric(dataset_2017[col])\r\n",
    "for col in dataset_2016:\r\n",
    "    dataset_2016[col]=pd.to_numeric(dataset_2016[col])\r\n",
    "for col in dataset_2015:\r\n",
    "    dataset_2015[col]=pd.to_numeric(dataset_2015[col])\r\n",
    "for col in dataset_2014:\r\n",
    "    dataset_2014[col]=pd.to_numeric(dataset_2014[col])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "source": [
    "close_2016=dataset_close[dataset_close['year_close_combined']==2016]\r\n",
    "close_2015=dataset_close[dataset_close['year_close_combined']==2015]\r\n",
    "close_2014=dataset_close[dataset_close['year_close_combined']==2014]\r\n",
    "close_2013=dataset_close[dataset_close['year_close_combined']==2013]\r\n",
    "dataset_2017['Closure']=dataset_2017.id.apply(lambda x:1 if x in np.array(close_2016['id']) else 0)\r\n",
    "dataset_2016['Closure']=dataset_2016.id.apply(lambda x:1 if x in np.array(close_2015['id']) else 0)\r\n",
    "dataset_2015['Closure']=dataset_2015.id.apply(lambda x:1 if x in np.array(close_2014['id']) else 0)\r\n",
    "dataset_2014['Closure']=dataset_2014.id.apply(lambda x:1 if x in np.array(close_2013['id']) else 0)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "source": [
    "dataset_2014[dataset_2014['Closure']==1]['id'].value_counts().sort_index()"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "10066     44\n",
       "30126     44\n",
       "50349     44\n",
       "50768     44\n",
       "140151    44\n",
       "360113    44\n",
       "450283    44\n",
       "450770    44\n",
       "450813    44\n",
       "450839    44\n",
       "490012    44\n",
       "Name: id, dtype: int64"
      ]
     },
     "metadata": {},
     "execution_count": 253
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "source": [
    "dataset_2016.loc[dataset_2016['id']==200025,'Closure']=0\r\n",
    "dataset_2014.loc[dataset_2014['id']==180149,'Closure']=0\r\n",
    "dataset_2014.loc[dataset_2014['id']==450832,'Closure']=0\r\n",
    "dataset_2014.loc[dataset_2014['id']==670052,'Closure']=0"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "source": [
    "dataset_2014['Closure'].value_counts(dropna=False)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0    142956\n",
       "1       484\n",
       "Name: Closure, dtype: int64"
      ]
     },
     "metadata": {},
     "execution_count": 254
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "source": [
    "dataset_2017=dataset_2017.to_numpy().reshape(int(dataset_2017.shape[0]/22),8*22)\r\n",
    "dataset_2016=dataset_2016.to_numpy().reshape(int(dataset_2016.shape[0]/22),8*22)\r\n",
    "dataset_2015=dataset_2015.to_numpy().reshape(int(dataset_2015.shape[0]/22),8*22)\r\n",
    "dataset_2014=dataset_2014.to_numpy().reshape(int(dataset_2014.shape[0]/22),8*22)\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "source": [
    "arr1=[i*8 for i in range(22)]\r\n",
    "arr2=[i*8+7 for i in range(22)]\r\n",
    "arr=np.array([arr1,arr2]).flatten()\r\n",
    "arr.sort()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "source": [
    "dataset_2017=np.delete(dataset_2017,arr[:-1],axis=1)\r\n",
    "dataset_2016=np.delete(dataset_2016,arr[:-1],axis=1)\r\n",
    "dataset_2015=np.delete(dataset_2015,arr[:-1],axis=1)\r\n",
    "dataset_2014=np.delete(dataset_2014,arr[:-1],axis=1)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "source": [
    "dataset_2014.shape"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(6520, 133)"
      ]
     },
     "metadata": {},
     "execution_count": 258
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "source": [
    "dataset_=np.concatenate((dataset_2014,dataset_2015,dataset_2016,dataset_2017),axis=0)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "source": [
    "dataset_.shape"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(16124, 133)"
      ]
     },
     "metadata": {},
     "execution_count": 260
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "source": [
    "train, test = train_test_split(dataset_, test_size=0.3)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "source": [
    "X_train = train[:,:-1]\r\n",
    "Y_train = train[:,-1].astype(int)\r\n",
    "X_test  = test[:,:-1]\r\n",
    "Y_test  = test[:,-1].astype(int)\r\n",
    "X_train.shape, Y_train.shape,X_test.shape, Y_test.shape"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "((11286, 132), (11286,), (4838, 132), (4838,))"
      ]
     },
     "metadata": {},
     "execution_count": 262
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "source": [
    "m=RandomForestClassifier()\r\n",
    "m.fit(X_train, Y_train)\r\n",
    "predict_y = m.predict(X_test)\r\n",
    "print('ROCAUC score:',roc_auc_score(Y_test, predict_y))\r\n",
    "print('Accuracy score:',accuracy_score(Y_test, predict_y))\r\n",
    "print('F1 score:',f1_score(Y_test, predict_y))\r\n",
    "print(classification_report(Y_test, predict_y))\r\n",
    "confusion_matrix(Y_test, predict_y)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "ROCAUC score: 0.71875\n",
      "Accuracy score: 0.9981397271599834\n",
      "F1 score: 0.6086956521739131\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      4822\n",
      "           1       1.00      0.44      0.61        16\n",
      "\n",
      "    accuracy                           1.00      4838\n",
      "   macro avg       1.00      0.72      0.80      4838\n",
      "weighted avg       1.00      1.00      1.00      4838\n",
      "\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([[4822,    0],\n",
       "       [   9,    7]], dtype=int64)"
      ]
     },
     "metadata": {},
     "execution_count": 263
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "source": [
    "\r\n",
    "\r\n",
    "smote = ADASYN()\r\n",
    "\r\n",
    "# fit target and predictor variable\r\n",
    "x_smote , y_smote = smote.fit_sample(X_train, Y_train)\r\n",
    "x_train1, y_train1 = x_smote , y_smote"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "source": [
    "x_train1, y_train1=loras.fit_resample()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "source": [
    "#scaler = StandardScaler()  \r\n",
    "\r\n",
    "#scaler.fit(x_train1)  \r\n",
    "#x_train1 = scaler.transform(x_train1)  \r\n",
    "\r\n",
    "#x_test1 = scaler.transform(X_test)\r\n",
    "x_test1 =X_test\r\n",
    "\r\n",
    "#m = MLPClassifier(hidden_layer_sizes=(132,132))\r\n",
    "m=LogisticRegression()\r\n",
    "m.fit(x_train1, y_train1)\r\n",
    "predict_y = m.predict(x_test1)\r\n",
    "\r\n",
    "print('ROCAUC score:',roc_auc_score(Y_test, predict_y))\r\n",
    "print('Accuracy score:',accuracy_score(Y_test, predict_y))\r\n",
    "print('F1 score:',f1_score(Y_test, predict_y))\r\n",
    "print(classification_report(Y_test, predict_y))\r\n",
    "confusion_matrix(Y_test, predict_y)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "ROCAUC score: 0.7718270427208627\n",
      "Accuracy score: 0.7935097147581646\n",
      "F1 score: 0.02346041055718475\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.79      0.88      4822\n",
      "           1       0.01      0.75      0.02        16\n",
      "\n",
      "    accuracy                           0.79      4838\n",
      "   macro avg       0.51      0.77      0.45      4838\n",
      "weighted avg       1.00      0.79      0.88      4838\n",
      "\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "C:\\Users\\zys\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([[3827,  995],\n",
       "       [   4,   12]], dtype=int64)"
      ]
     },
     "metadata": {},
     "execution_count": 265
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "source": [
    "m=RandomForestClassifier()\r\n",
    "m.fit(x_train1, y_train1)\r\n",
    "predict_y = m.predict(x_test1)\r\n",
    "print('ROCAUC score:',roc_auc_score(Y_test, predict_y))\r\n",
    "print('Accuracy score:',accuracy_score(Y_test, predict_y))\r\n",
    "print('F1 score:',f1_score(Y_test, predict_y))\r\n",
    "print(classification_report(Y_test, predict_y))\r\n",
    "confusion_matrix(Y_test, predict_y)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "ROCAUC score: 0.71875\n",
      "Accuracy score: 0.9981397271599834\n",
      "F1 score: 0.6086956521739131\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      4822\n",
      "           1       1.00      0.44      0.61        16\n",
      "\n",
      "    accuracy                           1.00      4838\n",
      "   macro avg       1.00      0.72      0.80      4838\n",
      "weighted avg       1.00      1.00      1.00      4838\n",
      "\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([[4822,    0],\n",
       "       [   9,    7]], dtype=int64)"
      ]
     },
     "metadata": {},
     "execution_count": 266
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(dataset_[:,:-1],\r\n",
    "                                                    dataset_[:,-1],\r\n",
    "                                                    test_size=0.3,\r\n",
    "                                                    stratify=dataset_[:,-1]\r\n",
    "                                                    )\r\n",
    "\r\n",
    "\r\n",
    "pipeline = imbpipeline(steps = [['smote', SMOTE()],\r\n",
    "                                ['scaler', MinMaxScaler()],\r\n",
    "                                ['classifier', RandomForestClassifier()]])\r\n",
    "\r\n",
    "stratified_kfold = StratifiedKFold(n_splits=5,\r\n",
    "                                       shuffle=True\r\n",
    "                                       )\r\n",
    "    \r\n",
    "param_grid = {'classifier__n_estimators':[10,20,30,40,50,60,70,80,90,100,150,200,300,400,500]}\r\n",
    "grid_search = GridSearchCV(estimator=pipeline,\r\n",
    "                           param_grid=param_grid,\r\n",
    "                           scoring='f1_macro',\r\n",
    "                           cv=stratified_kfold,\r\n",
    "                           n_jobs=-1)\r\n",
    "\r\n",
    "grid_search.fit(X_train, y_train)\r\n",
    "cv_score_RF_s = grid_search.best_score_\r\n",
    "test_score_RF_s = grid_search.score(X_test, y_test)\r\n",
    "print(f'Cross-validation score: {cv_score_RF_s}\\nTest score: {test_score_RF_s}')"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Cross-validation score: 0.689172202682504\n",
      "Test score: 0.6759011711113683\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "source": [
    "pipeline = imbpipeline(steps = [['ADASYN', ADASYN()],\r\n",
    "                                ['scaler', MinMaxScaler()],\r\n",
    "                                ['classifier', RandomForestClassifier()]])\r\n",
    "\r\n",
    "stratified_kfold = StratifiedKFold(n_splits=5,\r\n",
    "                                       shuffle=True\r\n",
    "                                       )\r\n",
    "    \r\n",
    "param_grid = {'classifier__n_estimators':[10,20,30,40,50,60,70,80,90,100,150,200,300,400,500]}\r\n",
    "grid_search = GridSearchCV(estimator=pipeline,\r\n",
    "                           param_grid=param_grid,\r\n",
    "                           scoring='f1_macro',\r\n",
    "                           cv=stratified_kfold,\r\n",
    "                           n_jobs=-1)\r\n",
    "\r\n",
    "grid_search.fit(X_train, y_train)\r\n",
    "cv_score_RF_a = grid_search.best_score_\r\n",
    "test_score_RF_a = grid_search.score(X_test, y_test)\r\n",
    "print(f'Cross-validation score: {cv_score_RF_a}\\nTest score: {test_score_RF_a}')"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Cross-validation score: 0.6510326187220229\n",
      "Test score: 0.7217045166931272\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "source": [
    "pipeline = imbpipeline(steps = [['SVMSMOTE', SVMSMOTE()],\r\n",
    "                                ['scaler', MinMaxScaler()],\r\n",
    "                                ['classifier', RandomForestClassifier()]])\r\n",
    "\r\n",
    "stratified_kfold = StratifiedKFold(n_splits=5,\r\n",
    "                                       shuffle=True\r\n",
    "                                       )\r\n",
    "    \r\n",
    "param_grid = {'classifier__n_estimators':[10,20,30,40,50,60,70,80,90,100,150,200,300,400,500]}\r\n",
    "grid_search = GridSearchCV(estimator=pipeline,\r\n",
    "                           param_grid=param_grid,\r\n",
    "                           scoring='f1_macro',\r\n",
    "                           cv=stratified_kfold,\r\n",
    "                           n_jobs=-1)\r\n",
    "\r\n",
    "grid_search.fit(X_train, y_train)\r\n",
    "cv_score_RF_svm = grid_search.best_score_\r\n",
    "test_score_RF_svm = grid_search.score(X_test, y_test)\r\n",
    "print(f'Cross-validation score: {cv_score_RF_svm}\\nTest score: {test_score_RF_svm}')"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Cross-validation score: 0.651032618722023\n",
      "Test score: 0.7217045166931272\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "source": [
    "pipeline = imbpipeline(steps = [['SMOTE', SMOTE()],\r\n",
    "                                ['scaler', MinMaxScaler()],\r\n",
    "                                ['classifier', LogisticRegression()]])\r\n",
    "    \r\n",
    "param_grid = {'classifier__C':[0.001, 0.01, 0.1, 1, 10, 100, 1000]}\r\n",
    "grid_search = GridSearchCV(estimator=pipeline,\r\n",
    "                           param_grid=param_grid,\r\n",
    "                           scoring='f1_macro',\r\n",
    "                           cv=stratified_kfold,\r\n",
    "                           n_jobs=-1)\r\n",
    "\r\n",
    "grid_search.fit(X_train, y_train)\r\n",
    "cv_score_LR_s = grid_search.best_score_\r\n",
    "test_score_LR_s= grid_search.score(X_test, y_test)\r\n",
    "print(f'Cross-validation score: {cv_score_LR_s}\\nTest score: {test_score_LR_s}')"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Cross-validation score: 0.4638708400596904\n",
      "Test score: 0.4599437600016514\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "C:\\Users\\zys\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "source": [
    "pipeline = imbpipeline(steps = [['ADASYN', ADASYN()],\r\n",
    "                                ['scaler', MinMaxScaler()],\r\n",
    "                                ['classifier', LogisticRegression()]])\r\n",
    "    \r\n",
    "param_grid = {'classifier__C':[0.001, 0.01, 0.1, 1, 10, 100, 1000]}\r\n",
    "grid_search = GridSearchCV(estimator=pipeline,\r\n",
    "                           param_grid=param_grid,\r\n",
    "                           scoring='f1_macro',\r\n",
    "                           cv=stratified_kfold,\r\n",
    "                           n_jobs=-1)\r\n",
    "\r\n",
    "grid_search.fit(X_train, y_train)\r\n",
    "cv_score_LR_a = grid_search.best_score_\r\n",
    "test_score_LR_a= grid_search.score(X_test, y_test)\r\n",
    "print(f'Cross-validation score: {cv_score_LR_a}\\nTest score: {test_score_LR_a}')"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Cross-validation score: 0.4641704005633172\n",
      "Test score: 0.45767509400495365\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "C:\\Users\\zys\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "source": [
    "pipeline = imbpipeline(steps = [['SVMSMOTE', SVMSMOTE()],\r\n",
    "                                ['scaler', MinMaxScaler()],\r\n",
    "                                ['classifier', LogisticRegression()]])\r\n",
    "    \r\n",
    "param_grid = {'classifier__C':[0.001, 0.01, 0.1, 1, 10, 100, 1000]}\r\n",
    "grid_search = GridSearchCV(estimator=pipeline,\r\n",
    "                           param_grid=param_grid,\r\n",
    "                           scoring='f1_macro',\r\n",
    "                           cv=stratified_kfold,\r\n",
    "                           n_jobs=-1)\r\n",
    "\r\n",
    "grid_search.fit(X_train, y_train)\r\n",
    "cv_score_LR_svm = grid_search.best_score_\r\n",
    "test_score_LR_svm= grid_search.score(X_test, y_test)\r\n",
    "print(f'Cross-validation score: {cv_score_LR_svm}\\nTest score: {test_score_LR_svm}')"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Cross-validation score: 0.5229216606259434\n",
      "Test score: 0.5631164400742488\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "source": [
    "pipeline = imbpipeline(steps = [['SMOTE', SMOTE()],\r\n",
    "                                ['scaler', MinMaxScaler()],\r\n",
    "                                ['classifier', DecisionTreeClassifier()]])\r\n",
    "    \r\n",
    "param_grid = {'classifier__criterion':['gini','entropy'],'classifier__max_depth':[4,5,6,7,8,9,10,11,12,15,20,30,40,50,70,90,120,150,200,300,400,500]}\r\n",
    "grid_search = GridSearchCV(estimator=pipeline,\r\n",
    "                           param_grid=param_grid,\r\n",
    "                           scoring='f1_macro',\r\n",
    "                           cv=stratified_kfold,\r\n",
    "                           n_jobs=-1)\r\n",
    "\r\n",
    "grid_search.fit(X_train, y_train)\r\n",
    "cv_score_DT_s = grid_search.best_score_\r\n",
    "test_score_DT_s= grid_search.score(X_test, y_test)\r\n",
    "print(f'Cross-validation score: {cv_score_DT_s}\\nTest score: {test_score_DT_s}')"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Cross-validation score: 0.6279275548703435\n",
      "Test score: 0.5912065839567957\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "source": [
    "pipeline = imbpipeline(steps = [['ADASYN', ADASYN()],\r\n",
    "                                ['scaler', MinMaxScaler()],\r\n",
    "                                ['classifier', DecisionTreeClassifier()]])\r\n",
    "    \r\n",
    "param_grid = {'classifier__criterion':['gini','entropy'],'classifier__max_depth':[4,5,6,7,8,9,10,11,12,15,20,30,40,50,70,90,120,150,200,300,400,500]}\r\n",
    "grid_search = GridSearchCV(estimator=pipeline,\r\n",
    "                           param_grid=param_grid,\r\n",
    "                           scoring='f1_macro',\r\n",
    "                           cv=stratified_kfold,\r\n",
    "                           n_jobs=-1)\r\n",
    "\r\n",
    "grid_search.fit(X_train, y_train)\r\n",
    "cv_score_DT_a = grid_search.best_score_\r\n",
    "test_score_DT_a= grid_search.score(X_test, y_test)\r\n",
    "print(f'Cross-validation score: {cv_score_DT_a}\\nTest score: {test_score_DT_a}')"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Cross-validation score: 0.6260475512366629\n",
      "Test score: 0.6010569263091474\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "source": [
    "pipeline = imbpipeline(steps = [['SVMSMOTE',SVMSMOTE()],\r\n",
    "                                ['scaler', MinMaxScaler()],\r\n",
    "                                ['classifier', DecisionTreeClassifier()]])\r\n",
    "    \r\n",
    "param_grid = {'classifier__criterion':['gini','entropy'],'classifier__max_depth':[4,5,6,7,8,9,10,11,12,15,20,30,40,50,70,90,120,150,200,300,400,500]}\r\n",
    "grid_search = GridSearchCV(estimator=pipeline,\r\n",
    "                           param_grid=param_grid,\r\n",
    "                           scoring='f1_macro',\r\n",
    "                           cv=stratified_kfold,\r\n",
    "                           n_jobs=-1)\r\n",
    "\r\n",
    "grid_search.fit(X_train, y_train)\r\n",
    "cv_score_DT_svm = grid_search.best_score_\r\n",
    "test_score_DT_svm= grid_search.score(X_test, y_test)\r\n",
    "print(f'Cross-validation score: {cv_score_DT_svm}\\nTest score: {test_score_DT_svm}')"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Cross-validation score: 0.6257767149497375\n",
      "Test score: 0.6096588289534348\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "source": [
    "pipeline = imbpipeline(steps = [['SMOTE', SMOTE()],\r\n",
    "                                ['scaler', MinMaxScaler()],\r\n",
    "                                ['classifier', KNeighborsClassifier()]])\r\n",
    "    \r\n",
    "param_grid = {'classifier__n_neighbors':[1,2,3,4,5,6,7,8,9,10,20,30,40,50,80,100,200,300,400,500,600,700,800,900,1000]}\r\n",
    "grid_search = GridSearchCV(estimator=pipeline,\r\n",
    "                           param_grid=param_grid,\r\n",
    "                           scoring='f1_macro',\r\n",
    "                           cv=stratified_kfold,\r\n",
    "                           n_jobs=-1)\r\n",
    "\r\n",
    "grid_search.fit(X_train, y_train)\r\n",
    "cv_score_knn_s = grid_search.best_score_\r\n",
    "test_score_knn_s= grid_search.score(X_test, y_test)\r\n",
    "print(f'Cross-validation score: {cv_score_knn_s}\\nTest score: {test_score_knn_s}')"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Cross-validation score: 0.5261014139250931\n",
      "Test score: 0.6156210847975554\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "source": [
    "pipeline = imbpipeline(steps = [['ADASYN', ADASYN()],\r\n",
    "                                ['scaler', MinMaxScaler()],\r\n",
    "                                ['classifier', KNeighborsClassifier()]])\r\n",
    "    \r\n",
    "param_grid = {'classifier__n_neighbors':[1,2,3,4,5,6,7,8,9,10,20,30,40,50,80,100,200,300,400,500,600,700,800,900,1000]}\r\n",
    "grid_search = GridSearchCV(estimator=pipeline,\r\n",
    "                           param_grid=param_grid,\r\n",
    "                           scoring='f1_macro',\r\n",
    "                           cv=stratified_kfold,\r\n",
    "                           n_jobs=-1)\r\n",
    "\r\n",
    "grid_search.fit(X_train, y_train)\r\n",
    "cv_score_knn_a = grid_search.best_score_\r\n",
    "test_score_knn_a= grid_search.score(X_test, y_test)\r\n",
    "print(f'Cross-validation score: {cv_score_knn_a}\\nTest score: {test_score_knn_a}')"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Cross-validation score: 0.5400147539820169\n",
      "Test score: 0.6180261790982755\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "source": [
    "pipeline = imbpipeline(steps = [['SVMSMOTE', SVMSMOTE()],\r\n",
    "                                ['scaler', MinMaxScaler()],\r\n",
    "                                ['classifier', KNeighborsClassifier()]])\r\n",
    "    \r\n",
    "param_grid = {'classifier__n_neighbors':[1,2,3,4,5,6,7,8,9,10,20,30,40,50,80,100,200,300,400,500,600,700,800,900,1000]}\r\n",
    "grid_search = GridSearchCV(estimator=pipeline,\r\n",
    "                           param_grid=param_grid,\r\n",
    "                           scoring='f1_macro',\r\n",
    "                           cv=stratified_kfold,\r\n",
    "                           n_jobs=-1)\r\n",
    "\r\n",
    "grid_search.fit(X_train, y_train)\r\n",
    "cv_score_knn_svm = grid_search.best_score_\r\n",
    "test_score_knn_svm= grid_search.score(X_test, y_test)\r\n",
    "print(f'Cross-validation score: {cv_score_knn_svm}\\nTest score: {test_score_knn_svm}')"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Cross-validation score: 0.5913627675686772\n",
      "Test score: 0.7214449396022616\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "source": [
    "pipeline = imbpipeline(steps = [['SMOTE', SMOTE()],\r\n",
    "                                ['scaler', MinMaxScaler()],\r\n",
    "                                ['classifier', XGBClassifier()]])\r\n",
    "    \r\n",
    "param_grid = {'classifier__min_child_weight': [1, 5, 10],\r\n",
    "        'classifier__max_depth': [3, 4, 5]}\r\n",
    "grid_search = GridSearchCV(estimator=pipeline,\r\n",
    "                           param_grid=param_grid,\r\n",
    "                           scoring='f1_macro',\r\n",
    "                           cv=stratified_kfold,\r\n",
    "                           n_jobs=-1)\r\n",
    "\r\n",
    "grid_search.fit(X_train, y_train)\r\n",
    "cv_score_xgb_s = grid_search.best_score_\r\n",
    "test_score_xgb_s= grid_search.score(X_test, y_test)\r\n",
    "print(f'Cross-validation score: {cv_score_xgb_s}\\nTest score: {test_score_xgb_s}')"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "C:\\Users\\zys\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[06:25:16] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "Cross-validation score: 0.6828230356731648\n",
      "Test score: 0.6993786246893123\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "source": [
    "pipeline = imbpipeline(steps = [['ADASYN', ADASYN()],\r\n",
    "                                ['scaler', MinMaxScaler()],\r\n",
    "                                ['classifier', XGBClassifier()]])\r\n",
    "\r\n",
    "grid_search = GridSearchCV(estimator=pipeline,\r\n",
    "                           param_grid=param_grid,\r\n",
    "                           scoring='f1_macro',\r\n",
    "                           cv=stratified_kfold,\r\n",
    "                           n_jobs=-1)\r\n",
    "\r\n",
    "grid_search.fit(X_train, y_train)\r\n",
    "cv_score_xgb_a = grid_search.best_score_\r\n",
    "test_score_xgb_a= grid_search.score(X_test, y_test)\r\n",
    "print(f'Cross-validation score: {cv_score_xgb_a}\\nTest score: {test_score_xgb_a}')"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "C:\\Users\\zys\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[06:27:31] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "Cross-validation score: 0.7010769449107859\n",
      "Test score: 0.7602997941617609\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "source": [
    "pipeline = imbpipeline(steps = [['SVMSMOTE', SVMSMOTE()],\r\n",
    "                                ['scaler', MinMaxScaler()],\r\n",
    "                                ['classifier', XGBClassifier()]])\r\n",
    "    \r\n",
    "grid_search = GridSearchCV(estimator=pipeline,\r\n",
    "                           param_grid=param_grid,\r\n",
    "                           scoring='f1_macro',\r\n",
    "                           cv=stratified_kfold,\r\n",
    "                           n_jobs=-1)\r\n",
    "\r\n",
    "grid_search.fit(X_train, y_train)\r\n",
    "cv_score_xgb_svm = grid_search.best_score_\r\n",
    "test_score_xgb_svm= grid_search.score(X_test, y_test)\r\n",
    "print(f'Cross-validation score: {cv_score_xgb_svm}\\nTest score: {test_score_xgb_svm}')"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "C:\\Users\\zys\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[06:28:13] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "Cross-validation score: 0.660159602849007\n",
      "Test score: 0.7217045166931272\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "source": [
    "models = pd.DataFrame({\r\n",
    "    'Model': ['Random Forest', 'Logistic Regression', \r\n",
    "              'Decision Tree','KNN',  \r\n",
    "              'XGBoost'],\r\n",
    "    'SMOTE': [cv_score_RF_s,cv_score_LR_s,cv_score_DT_s,cv_score_knn_s,cv_score_xgb_s],\r\n",
    "    'ADASYN': [cv_score_RF_a,cv_score_LR_a,cv_score_DT_a,cv_score_knn_a,cv_score_xgb_a],\r\n",
    "    'SVMSMOTE':[cv_score_RF_svm,cv_score_LR_svm,cv_score_DT_svm,cv_score_knn_svm,cv_score_xgb_svm]})"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "source": [
    "models"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "                 Model     SMOTE    ADASYN  SVMSMOTE\n",
       "0        Random Forest  0.689172  0.651033  0.651033\n",
       "1  Logistic Regression  0.463871  0.464170  0.522922\n",
       "2        Decision Tree  0.627928  0.626048  0.625777\n",
       "3                  KNN  0.526101  0.540015  0.591363\n",
       "4              XGBoost  0.682823  0.701077  0.660160"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>SMOTE</th>\n",
       "      <th>ADASYN</th>\n",
       "      <th>SVMSMOTE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Random Forest</td>\n",
       "      <td>0.689172</td>\n",
       "      <td>0.651033</td>\n",
       "      <td>0.651033</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>0.463871</td>\n",
       "      <td>0.464170</td>\n",
       "      <td>0.522922</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Decision Tree</td>\n",
       "      <td>0.627928</td>\n",
       "      <td>0.626048</td>\n",
       "      <td>0.625777</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>KNN</td>\n",
       "      <td>0.526101</td>\n",
       "      <td>0.540015</td>\n",
       "      <td>0.591363</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>XGBoost</td>\n",
       "      <td>0.682823</td>\n",
       "      <td>0.701077</td>\n",
       "      <td>0.660160</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "metadata": {},
     "execution_count": 291
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# To do:\r\n",
    "1. add features\r\n",
    "2. more algorithms\r\n",
    "3. bayesian optimisation\r\n",
    "4. oversampling strategy?"
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}