{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import pandas as pd\r\n",
    "import numpy as np\r\n",
    "import warnings\r\n",
    "import matplotlib.pyplot as plt\r\n",
    "import pyodbc\r\n",
    "import pandas_access as pa\r\n",
    "from sklearn.model_selection import train_test_split\r\n",
    "from sklearn.linear_model import LogisticRegression\r\n",
    "from sklearn.svm import SVC, LinearSVC\r\n",
    "from sklearn.ensemble import RandomForestClassifier\r\n",
    "from sklearn.neighbors import KNeighborsClassifier\r\n",
    "from sklearn.naive_bayes import GaussianNB\r\n",
    "from sklearn.linear_model import Perceptron\r\n",
    "from sklearn.linear_model import SGDClassifier\r\n",
    "from sklearn.tree import DecisionTreeClassifier\r\n",
    "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix, roc_auc_score,classification_report\r\n",
    "from sklearn.model_selection import cross_val_predict,GridSearchCV, StratifiedKFold\r\n",
    "from sklearn.ensemble import IsolationForest\r\n",
    "from sklearn.svm import OneClassSVM\r\n",
    "from sklearn.covariance import EllipticEnvelope\r\n",
    "from xgboost import XGBClassifier\r\n",
    "from sklearn.preprocessing import StandardScaler,MinMaxScaler  \r\n",
    "from sklearn.neural_network import MLPClassifier\r\n",
    "from tensorflow import keras\r\n",
    "import tensorflow as tf\r\n",
    "import imblearn\r\n",
    "from imblearn.over_sampling import SMOTE,BorderlineSMOTE,ADASYN,SVMSMOTE\r\n",
    "from imblearn.under_sampling import RandomUnderSampler\r\n",
    "from imblearn.pipeline import Pipeline as imbpipeline\r\n",
    "import seaborn as sns\r\n",
    "from collections import Counter\r\n",
    "import loras"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "pd.set_option('display.max_columns',200)\r\n",
    "pd.set_option('display.max_rows',200)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "dataset_close=pd.read_csv('data\\hospital_closure\\hospital_closure.csv')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "source": [
    "list(cur_2017.tables())"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[('data\\\\hospital_compare\\\\hc_apr2017\\\\Hospital.mdb', None, 'MSysAccessObjects', 'SYSTEM TABLE', None),\n",
       " ('data\\\\hospital_compare\\\\hc_apr2017\\\\Hospital.mdb', None, 'MSysAccessXML', 'SYSTEM TABLE', None),\n",
       " ('data\\\\hospital_compare\\\\hc_apr2017\\\\Hospital.mdb', None, 'MSysACEs', 'SYSTEM TABLE', None),\n",
       " ('data\\\\hospital_compare\\\\hc_apr2017\\\\Hospital.mdb', None, 'MSysIMEXColumns', 'SYSTEM TABLE', None),\n",
       " ('data\\\\hospital_compare\\\\hc_apr2017\\\\Hospital.mdb', None, 'MSysIMEXSpecs', 'SYSTEM TABLE', None),\n",
       " ('data\\\\hospital_compare\\\\hc_apr2017\\\\Hospital.mdb', None, 'MSysNameMap', 'SYSTEM TABLE', None),\n",
       " ('data\\\\hospital_compare\\\\hc_apr2017\\\\Hospital.mdb', None, 'MSysNavPaneGroupCategories', 'SYSTEM TABLE', None),\n",
       " ('data\\\\hospital_compare\\\\hc_apr2017\\\\Hospital.mdb', None, 'MSysNavPaneGroups', 'SYSTEM TABLE', None),\n",
       " ('data\\\\hospital_compare\\\\hc_apr2017\\\\Hospital.mdb', None, 'MSysNavPaneGroupToObjects', 'SYSTEM TABLE', None),\n",
       " ('data\\\\hospital_compare\\\\hc_apr2017\\\\Hospital.mdb', None, 'MSysNavPaneObjectIDs', 'SYSTEM TABLE', None),\n",
       " ('data\\\\hospital_compare\\\\hc_apr2017\\\\Hospital.mdb', None, 'MSysObjects', 'SYSTEM TABLE', None),\n",
       " ('data\\\\hospital_compare\\\\hc_apr2017\\\\Hospital.mdb', None, 'MSysQueries', 'SYSTEM TABLE', None),\n",
       " ('data\\\\hospital_compare\\\\hc_apr2017\\\\Hospital.mdb', None, 'MSysRelationships', 'SYSTEM TABLE', None),\n",
       " ('data\\\\hospital_compare\\\\hc_apr2017\\\\Hospital.mdb', None, 'Ambulatory Surgical Measures-Facility', 'TABLE', None),\n",
       " ('data\\\\hospital_compare\\\\hc_apr2017\\\\Hospital.mdb', None, 'Ambulatory Surgical Measures-National', 'TABLE', None),\n",
       " ('data\\\\hospital_compare\\\\hc_apr2017\\\\Hospital.mdb', None, 'Ambulatory Surgical Measures-State', 'TABLE', None),\n",
       " ('data\\\\hospital_compare\\\\hc_apr2017\\\\Hospital.mdb', None, 'FY2015_Distribution_of_Net_Change_in_Base_Op_DRG_Payment_Amt', 'TABLE', None),\n",
       " ('data\\\\hospital_compare\\\\hc_apr2017\\\\Hospital.mdb', None, 'FY2015_Net_Change_in_Base_Op_DRG_Payment_Amt', 'TABLE', None),\n",
       " ('data\\\\hospital_compare\\\\hc_apr2017\\\\Hospital.mdb', None, 'FY2015_Percent_Change_in_Medicare_Payments', 'TABLE', None),\n",
       " ('data\\\\hospital_compare\\\\hc_apr2017\\\\Hospital.mdb', None, 'FY2015_Value_Based_Incentive_Payment_Amount', 'TABLE', None),\n",
       " ('data\\\\hospital_compare\\\\hc_apr2017\\\\Hospital.mdb', None, 'GLOBAL_April2017_09March2017', 'TABLE', None),\n",
       " ('data\\\\hospital_compare\\\\hc_apr2017\\\\Hospital.mdb', None, 'HOSPITAL_QUARTERLY_HAC_DOMAIN_HOSPITAL', 'TABLE', None),\n",
       " ('data\\\\hospital_compare\\\\hc_apr2017\\\\Hospital.mdb', None, 'HOSPITAL_QUARTERLY_IPFQR_MEASURES_HOSPITAL', 'TABLE', None),\n",
       " ('data\\\\hospital_compare\\\\hc_apr2017\\\\Hospital.mdb', None, 'HOSPITAL_QUARTERLY_IPFQR_MEASURES_NATIONAL', 'TABLE', None),\n",
       " ('data\\\\hospital_compare\\\\hc_apr2017\\\\Hospital.mdb', None, 'HOSPITAL_QUARTERLY_IPFQR_MEASURES_STATE', 'TABLE', None),\n",
       " ('data\\\\hospital_compare\\\\hc_apr2017\\\\Hospital.mdb', None, 'HOSPITAL_QUARTERLY_MSPB_6_DECIMALS', 'TABLE', None),\n",
       " ('data\\\\hospital_compare\\\\hc_apr2017\\\\Hospital.mdb', None, 'HOSPITAL_QUARTERLY_QUALITYMEASURE_PCH_HCAHPS_HOSPITAL', 'TABLE', None),\n",
       " ('data\\\\hospital_compare\\\\hc_apr2017\\\\Hospital.mdb', None, 'HOSPITAL_QUARTERLY_QUALITYMEASURE_PCH_HCAHPS_NATIONAL', 'TABLE', None),\n",
       " ('data\\\\hospital_compare\\\\hc_apr2017\\\\Hospital.mdb', None, 'HOSPITAL_QUARTERLY_QUALITYMEASURE_PCH_HCAHPS_STATE', 'TABLE', None),\n",
       " ('data\\\\hospital_compare\\\\hc_apr2017\\\\Hospital.mdb', None, 'HOSPITAL_QUARTERLY_QUALITYMEASURE_PCH_HOSPITAL', 'TABLE', None),\n",
       " ('data\\\\hospital_compare\\\\hc_apr2017\\\\Hospital.mdb', None, 'HOSPITAL_QUARTERLY_QUALITYMEASURE_PCH_OCM_HOSPITAL', 'TABLE', None),\n",
       " ('data\\\\hospital_compare\\\\hc_apr2017\\\\Hospital.mdb', None, 'HQI_FTNT', 'TABLE', None),\n",
       " ('data\\\\hospital_compare\\\\hc_apr2017\\\\Hospital.mdb', None, 'HQI_HOSP', 'TABLE', None),\n",
       " ('data\\\\hospital_compare\\\\hc_apr2017\\\\Hospital.mdb', None, 'HQI_HOSP_Comp', 'TABLE', None),\n",
       " ('data\\\\hospital_compare\\\\hc_apr2017\\\\Hospital.mdb', None, 'HQI_HOSP_HAI', 'TABLE', None),\n",
       " ('data\\\\hospital_compare\\\\hc_apr2017\\\\Hospital.mdb', None, 'HQI_HOSP_HCAHPS', 'TABLE', None),\n",
       " ('data\\\\hospital_compare\\\\hc_apr2017\\\\Hospital.mdb', None, 'HQI_HOSP_IMG', 'TABLE', None),\n",
       " ('data\\\\hospital_compare\\\\hc_apr2017\\\\Hospital.mdb', None, 'HQI_HOSP_MSPB', 'TABLE', None),\n",
       " ('data\\\\hospital_compare\\\\hc_apr2017\\\\Hospital.mdb', None, 'HQI_HOSP_PaymentAndValueOfCare', 'TABLE', None),\n",
       " ('data\\\\hospital_compare\\\\hc_apr2017\\\\Hospital.mdb', None, 'HQI_HOSP_ReadmDeath', 'TABLE', None),\n",
       " ('data\\\\hospital_compare\\\\hc_apr2017\\\\Hospital.mdb', None, 'HQI_HOSP_STRUCTURAL', 'TABLE', None),\n",
       " ('data\\\\hospital_compare\\\\hc_apr2017\\\\Hospital.mdb', None, 'HQI_HOSP_TimelyEffectiveCare', 'TABLE', None),\n",
       " ('data\\\\hospital_compare\\\\hc_apr2017\\\\Hospital.mdb', None, 'HQI_NATIONAL_Comp', 'TABLE', None),\n",
       " ('data\\\\hospital_compare\\\\hc_apr2017\\\\Hospital.mdb', None, 'HQI_NATIONAL_HAI', 'TABLE', None),\n",
       " ('data\\\\hospital_compare\\\\hc_apr2017\\\\Hospital.mdb', None, 'HQI_NATIONAL_HCAHPS', 'TABLE', None),\n",
       " ('data\\\\hospital_compare\\\\hc_apr2017\\\\Hospital.mdb', None, 'HQI_NATIONAL_IMG_AVG', 'TABLE', None),\n",
       " ('data\\\\hospital_compare\\\\hc_apr2017\\\\Hospital.mdb', None, 'HQI_NATIONAL_MSPB', 'TABLE', None),\n",
       " ('data\\\\hospital_compare\\\\hc_apr2017\\\\Hospital.mdb', None, 'HQI_NATIONAL_Payment', 'TABLE', None),\n",
       " ('data\\\\hospital_compare\\\\hc_apr2017\\\\Hospital.mdb', None, 'HQI_NATIONAL_ReadmDeath', 'TABLE', None),\n",
       " ('data\\\\hospital_compare\\\\hc_apr2017\\\\Hospital.mdb', None, 'HQI_NATIONAL_TimelyEffectiveCare', 'TABLE', None),\n",
       " ('data\\\\hospital_compare\\\\hc_apr2017\\\\Hospital.mdb', None, 'HQI_NATIONAL_Value of Care', 'TABLE', None),\n",
       " ('data\\\\hospital_compare\\\\hc_apr2017\\\\Hospital.mdb', None, 'HQI_STATE_Comp', 'TABLE', None),\n",
       " ('data\\\\hospital_compare\\\\hc_apr2017\\\\Hospital.mdb', None, 'HQI_STATE_HAI', 'TABLE', None),\n",
       " ('data\\\\hospital_compare\\\\hc_apr2017\\\\Hospital.mdb', None, 'HQI_STATE_HCAHPS', 'TABLE', None),\n",
       " ('data\\\\hospital_compare\\\\hc_apr2017\\\\Hospital.mdb', None, 'HQI_STATE_IMG_AVG', 'TABLE', None),\n",
       " ('data\\\\hospital_compare\\\\hc_apr2017\\\\Hospital.mdb', None, 'HQI_STATE_MSPB', 'TABLE', None),\n",
       " ('data\\\\hospital_compare\\\\hc_apr2017\\\\Hospital.mdb', None, 'HQI_STATE_Payment', 'TABLE', None),\n",
       " ('data\\\\hospital_compare\\\\hc_apr2017\\\\Hospital.mdb', None, 'HQI_STATE_ReadmDeath', 'TABLE', None),\n",
       " ('data\\\\hospital_compare\\\\hc_apr2017\\\\Hospital.mdb', None, 'HQI_STATE_TimelyEffectiveCare', 'TABLE', None),\n",
       " ('data\\\\hospital_compare\\\\hc_apr2017\\\\Hospital.mdb', None, 'Hvbp_ami_11_14_2016', 'TABLE', None),\n",
       " ('data\\\\hospital_compare\\\\hc_apr2017\\\\Hospital.mdb', None, 'Hvbp_clinical_care_outcomes_11_10_2016', 'TABLE', None),\n",
       " ('data\\\\hospital_compare\\\\hc_apr2017\\\\Hospital.mdb', None, 'Hvbp_efficiency_11_10_2016', 'TABLE', None),\n",
       " ('data\\\\hospital_compare\\\\hc_apr2017\\\\Hospital.mdb', None, 'Hvbp_hcahps_11_10_2016', 'TABLE', None),\n",
       " ('data\\\\hospital_compare\\\\hc_apr2017\\\\Hospital.mdb', None, 'Hvbp_imm2_11_10_2016', 'TABLE', None),\n",
       " ('data\\\\hospital_compare\\\\hc_apr2017\\\\Hospital.mdb', None, 'Hvbp_pc_11_10_2016', 'TABLE', None),\n",
       " ('data\\\\hospital_compare\\\\hc_apr2017\\\\Hospital.mdb', None, 'Hvbp_safety_11_10_2016', 'TABLE', None),\n",
       " ('data\\\\hospital_compare\\\\hc_apr2017\\\\Hospital.mdb', None, 'Hvbp_tps_11_10_2016', 'TABLE', None),\n",
       " ('data\\\\hospital_compare\\\\hc_apr2017\\\\Hospital.mdb', None, 'Measure_Dates', 'TABLE', None),\n",
       " ('data\\\\hospital_compare\\\\hc_apr2017\\\\Hospital.mdb', None, 'Medicare Hospital Spending by Claim', 'TABLE', None),\n",
       " ('data\\\\hospital_compare\\\\hc_apr2017\\\\Hospital.mdb', None, 'MORT_READM_April2017', 'TABLE', None),\n",
       " ('data\\\\hospital_compare\\\\hc_apr2017\\\\Hospital.mdb', None, 'Outpatient Procedures - Volume', 'TABLE', None),\n",
       " ('data\\\\hospital_compare\\\\hc_apr2017\\\\Hospital.mdb', None, 'PSI_April2017', 'TABLE', None),\n",
       " ('data\\\\hospital_compare\\\\hc_apr2017\\\\Hospital.mdb', None, 'VA_HBIPS_December2016_CMS_Submission', 'TABLE', None),\n",
       " ('data\\\\hospital_compare\\\\hc_apr2017\\\\Hospital.mdb', None, 'VA_IPSHEP_Apr2017CMS_09MAR17', 'TABLE', None),\n",
       " ('data\\\\hospital_compare\\\\hc_apr2017\\\\Hospital.mdb', None, 'vwHQI_READM_REDUCTION', 'TABLE', None),\n",
       " ('data\\\\hospital_compare\\\\hc_apr2017\\\\Hospital.mdb', None, 'HQI_HOSP_MSPB Query', 'VIEW', None)]"
      ]
     },
     "metadata": {},
     "execution_count": 32
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "MDB_2017='data\\hospital_compare\\hc_apr2017\\Hospital.mdb'\r\n",
    "MDB_2016 = 'data\\hospital_compare\\hc_may2016\\Hospital.mdb'\r\n",
    "MDB_2015='data\\hospital_compare\\Copy of HospitalMay2015.mdb'\r\n",
    "MDB_2014='data\\hospital_compare\\HC_May2014.mdb'\r\n",
    "\r\n",
    "DRV = '{Microsoft Access Driver (*.mdb, *.accdb)}'\r\n",
    "PWD = 'pw'\r\n",
    "\r\n",
    "con_2017 = pyodbc.connect('DRIVER={};DBQ={};PWD={}'.format(DRV,MDB_2017,PWD))\r\n",
    "con_2016 = pyodbc.connect('DRIVER={};DBQ={};PWD={}'.format(DRV,MDB_2016,PWD))\r\n",
    "con_2015 = pyodbc.connect('DRIVER={};DBQ={};PWD={}'.format(DRV,MDB_2015,PWD))\r\n",
    "con_2014 = pyodbc.connect('DRIVER={};DBQ={};PWD={}'.format(DRV,MDB_2014,PWD))\r\n",
    "cur_2017 = con_2017.cursor()\r\n",
    "cur_2016 = con_2016.cursor()\r\n",
    "cur_2015 = con_2015.cursor()\r\n",
    "cur_2014 = con_2014.cursor()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "table_name = '[Medicare Hospital Spending by Claim]'\r\n",
    "\r\n",
    "query = \"SELECT * FROM {}\".format(table_name)\r\n",
    "\r\n",
    "query3 = \"SELECT * FROM {}.COLUMNS\".format(table_name)\r\n",
    "rows_2017 = cur_2017.execute(query).fetchall()\r\n",
    "rows_2016 = cur_2016.execute(query).fetchall()\r\n",
    "rows_2015 = cur_2015.execute(query).fetchall()\r\n",
    "rows_2014 = cur_2014.execute(query).fetchall()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "cur_2016.execute(query).fetchone()"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "('SOUTHEAST ALABAMA MEDICAL CENTER', '010001', 'AL', '1 to 3 days Prior to Index Hospital Admission', 'Home Health Agency', '$12', '$14', '$13', '0.06%', '0.07%', '0.07%', '01/01/2014', '12/31/2014')"
      ]
     },
     "metadata": {},
     "execution_count": 7
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "for row in cur_2017.columns(table='Medicare Hospital Spending by Claim'):\r\n",
    "    print(row.column_name)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Hospital_Name\n",
      "Provider_ID\n",
      "State\n",
      "Period\n",
      "Claim_Type\n",
      "Avg_Spending_Per_Episode_Hospital\n",
      "Avg_Spending_Per_Episode_State\n",
      "Avg_Spending_Per_Episode_Nation\n",
      "Percent_of_Spending_Hospital\n",
      "Percent_of_Spending_State\n",
      "Percent_of_Spending_Nation\n",
      "Start_Date\n",
      "End_Date\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "arr_2017=np.array(rows_2017)\r\n",
    "dataset_2017=pd.DataFrame(arr_2017[:,[1,5,6,7,8,9,10]],columns=['id','1','2','3','4','5','6'])\r\n",
    "arr_2016=np.array(rows_2016)\r\n",
    "dataset_2016=pd.DataFrame(arr_2016[:,[1,5,6,7,8,9,10]],columns=['id','1','2','3','4','5','6'])\r\n",
    "arr_2015=np.array(rows_2015)\r\n",
    "dataset_2015=pd.DataFrame(arr_2015[:,[1,5,6,7,8,9,10]],columns=['id','1','2','3','4','5','6'])\r\n",
    "arr_2014=np.array(rows_2014)\r\n",
    "dataset_2014=pd.DataFrame(arr_2014[:,[1,5,6,7,8,9,10]],columns=['id','1','2','3','4','5','6'])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "dataset_2014"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "            id         1         2         3        4        5        6\n",
       "0       010001       $11       $14       $13    0.06%    0.08%    0.07%\n",
       "1       010001        $4        $2        $1    0.02%    0.01%    0.00%\n",
       "2       010001        $9        $5        $5    0.05%    0.03%    0.03%\n",
       "3       010001       $71       $51       $63    0.38%    0.29%    0.34%\n",
       "4       010001        $1        $2        $2    0.01%    0.01%    0.01%\n",
       "...        ...       ...       ...       ...      ...      ...      ...\n",
       "143435  670082     $537      $647      $624     2.92%    3.25%    3.33%\n",
       "143436  670082   $2,682    $2,516    $2,924    14.56%   12.65%   15.63%\n",
       "143437  670082     $135      $132      $112     0.73%    0.66%    0.60%\n",
       "143438  670082   $1,228    $1,124    $1,005     6.67%    5.65%    5.37%\n",
       "143439  670082  $18,413   $19,886   $18,704   100.00%  100.00%  100.00%\n",
       "\n",
       "[143440 rows x 7 columns]"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>010001</td>\n",
       "      <td>$11</td>\n",
       "      <td>$14</td>\n",
       "      <td>$13</td>\n",
       "      <td>0.06%</td>\n",
       "      <td>0.08%</td>\n",
       "      <td>0.07%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>010001</td>\n",
       "      <td>$4</td>\n",
       "      <td>$2</td>\n",
       "      <td>$1</td>\n",
       "      <td>0.02%</td>\n",
       "      <td>0.01%</td>\n",
       "      <td>0.00%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>010001</td>\n",
       "      <td>$9</td>\n",
       "      <td>$5</td>\n",
       "      <td>$5</td>\n",
       "      <td>0.05%</td>\n",
       "      <td>0.03%</td>\n",
       "      <td>0.03%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>010001</td>\n",
       "      <td>$71</td>\n",
       "      <td>$51</td>\n",
       "      <td>$63</td>\n",
       "      <td>0.38%</td>\n",
       "      <td>0.29%</td>\n",
       "      <td>0.34%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>010001</td>\n",
       "      <td>$1</td>\n",
       "      <td>$2</td>\n",
       "      <td>$2</td>\n",
       "      <td>0.01%</td>\n",
       "      <td>0.01%</td>\n",
       "      <td>0.01%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>143435</th>\n",
       "      <td>670082</td>\n",
       "      <td>$537</td>\n",
       "      <td>$647</td>\n",
       "      <td>$624</td>\n",
       "      <td>2.92%</td>\n",
       "      <td>3.25%</td>\n",
       "      <td>3.33%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>143436</th>\n",
       "      <td>670082</td>\n",
       "      <td>$2,682</td>\n",
       "      <td>$2,516</td>\n",
       "      <td>$2,924</td>\n",
       "      <td>14.56%</td>\n",
       "      <td>12.65%</td>\n",
       "      <td>15.63%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>143437</th>\n",
       "      <td>670082</td>\n",
       "      <td>$135</td>\n",
       "      <td>$132</td>\n",
       "      <td>$112</td>\n",
       "      <td>0.73%</td>\n",
       "      <td>0.66%</td>\n",
       "      <td>0.60%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>143438</th>\n",
       "      <td>670082</td>\n",
       "      <td>$1,228</td>\n",
       "      <td>$1,124</td>\n",
       "      <td>$1,005</td>\n",
       "      <td>6.67%</td>\n",
       "      <td>5.65%</td>\n",
       "      <td>5.37%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>143439</th>\n",
       "      <td>670082</td>\n",
       "      <td>$18,413</td>\n",
       "      <td>$19,886</td>\n",
       "      <td>$18,704</td>\n",
       "      <td>100.00%</td>\n",
       "      <td>100.00%</td>\n",
       "      <td>100.00%</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>143440 rows × 7 columns</p>\n",
       "</div>"
      ]
     },
     "metadata": {},
     "execution_count": 10
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "dataset_2017['1']=dataset_2017['1'].str.strip(\"$\")\r\n",
    "dataset_2017['1']=dataset_2017['1'].str.replace(',', '')\r\n",
    "dataset_2017['2']=dataset_2017['2'].str.strip(\"$\")\r\n",
    "dataset_2017['2']=dataset_2017['2'].str.replace(',', '')\r\n",
    "dataset_2017['3']=dataset_2017['3'].str.strip(\"$\")\r\n",
    "dataset_2017['3']=dataset_2017['3'].str.replace(',', '')\r\n",
    "dataset_2016['1']=dataset_2016['1'].str.strip(\"$\")\r\n",
    "dataset_2016['1']=dataset_2016['1'].str.replace(',', '')\r\n",
    "dataset_2016['2']=dataset_2016['2'].str.strip(\"$\")\r\n",
    "dataset_2016['2']=dataset_2016['2'].str.replace(',', '')\r\n",
    "dataset_2016['3']=dataset_2016['3'].str.strip(\"$\")\r\n",
    "dataset_2016['3']=dataset_2016['3'].str.replace(',', '')\r\n",
    "dataset_2015['1']=dataset_2015['1'].str.strip(\"$\")\r\n",
    "dataset_2015['1']=dataset_2015['1'].str.replace(',', '')\r\n",
    "dataset_2015['2']=dataset_2015['2'].str.strip(\"$\")\r\n",
    "dataset_2015['2']=dataset_2015['2'].str.replace(',', '')\r\n",
    "dataset_2015['3']=dataset_2015['3'].str.strip(\"$\")\r\n",
    "dataset_2015['3']=dataset_2015['3'].str.replace(',', '')\r\n",
    "dataset_2014['1']=dataset_2014['1'].str.strip(\"$\")\r\n",
    "dataset_2014['1']=dataset_2014['1'].str.replace(',', '')\r\n",
    "dataset_2014['2']=dataset_2014['2'].str.strip(\"$\")\r\n",
    "dataset_2014['2']=dataset_2014['2'].str.replace(',', '')\r\n",
    "dataset_2014['3']=dataset_2014['3'].str.strip(\"$\")\r\n",
    "dataset_2014['3']=dataset_2014['3'].str.replace(',', '')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "dataset_2017['4']=dataset_2017['4'].str.strip(\"%\").astype(float)/100\r\n",
    "dataset_2017['5']=dataset_2017['5'].str.strip(\"%\").astype(float)/100\r\n",
    "dataset_2017['6']=dataset_2017['6'].str.strip(\"%\").astype(float)/100\r\n",
    "dataset_2016['4']=dataset_2016['4'].str.strip(\"%\").astype(float)/100\r\n",
    "dataset_2016['5']=dataset_2016['5'].str.strip(\"%\").astype(float)/100\r\n",
    "dataset_2016['6']=dataset_2016['6'].str.strip(\"%\").astype(float)/100\r\n",
    "dataset_2015['4']=dataset_2015['4'].str.strip(\"%\").astype(float)/100\r\n",
    "dataset_2015['5']=dataset_2015['5'].str.strip(\"%\").astype(float)/100\r\n",
    "dataset_2015['6']=dataset_2015['6'].str.strip(\"%\").astype(float)/100\r\n",
    "dataset_2014['4']=dataset_2014['4'].str.strip(\"%\").astype(float)/100\r\n",
    "dataset_2014['5']=dataset_2014['5'].str.strip(\"%\").astype(float)/100\r\n",
    "dataset_2014['6']=dataset_2014['6'].str.strip(\"%\").astype(float)/100"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "for col in dataset_2017:\r\n",
    "    dataset_2017[col]=pd.to_numeric(dataset_2017[col])\r\n",
    "for col in dataset_2016:\r\n",
    "    dataset_2016[col]=pd.to_numeric(dataset_2016[col])\r\n",
    "for col in dataset_2015:\r\n",
    "    dataset_2015[col]=pd.to_numeric(dataset_2015[col])\r\n",
    "for col in dataset_2014:\r\n",
    "    dataset_2014[col]=pd.to_numeric(dataset_2014[col])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "source": [
    "close_2016=dataset_close[dataset_close['year_close_combined']==2016]\r\n",
    "close_2015=dataset_close[dataset_close['year_close_combined']==2015]\r\n",
    "close_2014=dataset_close[dataset_close['year_close_combined']==2014]\r\n",
    "close_2013=dataset_close[dataset_close['year_close_combined']==2013]\r\n",
    "dataset_2017['Closure']=dataset_2017.id.apply(lambda x:1 if x in np.array(close_2016['id']) else 0)\r\n",
    "dataset_2016['Closure']=dataset_2016.id.apply(lambda x:1 if x in np.array(close_2015['id']) else 0)\r\n",
    "dataset_2015['Closure']=dataset_2015.id.apply(lambda x:1 if x in np.array(close_2014['id']) else 0)\r\n",
    "dataset_2014['Closure']=dataset_2014.id.apply(lambda x:1 if x in np.array(close_2013['id']) else 0)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "source": [
    "dataset_2014[dataset_2014['Closure']==1]['id'].value_counts().sort_index()"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "10066     44\n",
       "30126     44\n",
       "50349     44\n",
       "50768     44\n",
       "140151    44\n",
       "180149    44\n",
       "360113    44\n",
       "450283    44\n",
       "450770    44\n",
       "450813    44\n",
       "450832    44\n",
       "450839    44\n",
       "490012    44\n",
       "670052    44\n",
       "Name: id, dtype: int64"
      ]
     },
     "metadata": {},
     "execution_count": 15
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "source": [
    "dataset_2016.loc[dataset_2016['id']==200025,'Closure']=0\r\n",
    "dataset_2014.loc[dataset_2014['id']==180149,'Closure']=0\r\n",
    "dataset_2014.loc[dataset_2014['id']==450832,'Closure']=0\r\n",
    "dataset_2014.loc[dataset_2014['id']==670052,'Closure']=0"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "source": [
    "dataset_2014['Closure'].value_counts(dropna=False)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0    142956\n",
       "1       484\n",
       "Name: Closure, dtype: int64"
      ]
     },
     "metadata": {},
     "execution_count": 17
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "source": [
    "dataset_2017=dataset_2017.to_numpy().reshape(int(dataset_2017.shape[0]/22),8*22)\r\n",
    "dataset_2016=dataset_2016.to_numpy().reshape(int(dataset_2016.shape[0]/22),8*22)\r\n",
    "dataset_2015=dataset_2015.to_numpy().reshape(int(dataset_2015.shape[0]/22),8*22)\r\n",
    "dataset_2014=dataset_2014.to_numpy().reshape(int(dataset_2014.shape[0]/22),8*22)\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "source": [
    "arr1=[i*8 for i in range(22)]\r\n",
    "arr2=[i*8+7 for i in range(22)]\r\n",
    "arr=np.array([arr1,arr2]).flatten()\r\n",
    "arr.sort()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "source": [
    "dataset_2017=np.delete(dataset_2017,arr[:-1],axis=1)\r\n",
    "dataset_2016=np.delete(dataset_2016,arr[:-1],axis=1)\r\n",
    "dataset_2015=np.delete(dataset_2015,arr[:-1],axis=1)\r\n",
    "dataset_2014=np.delete(dataset_2014,arr[:-1],axis=1)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "source": [
    "dataset_2014.shape"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(6520, 133)"
      ]
     },
     "metadata": {},
     "execution_count": 21
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "source": [
    "dataset_=np.concatenate((dataset_2014,dataset_2015,dataset_2016,dataset_2017),axis=0)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "source": [
    "dataset_.shape"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(16124, 133)"
      ]
     },
     "metadata": {},
     "execution_count": 23
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "source": [
    "train, test = train_test_split(dataset_, test_size=0.3)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "source": [
    "X_train = train[:,:-1]\r\n",
    "Y_train = train[:,-1].astype(int)\r\n",
    "X_test  = test[:,:-1]\r\n",
    "Y_test  = test[:,-1].astype(int)\r\n",
    "X_train.shape, Y_train.shape,X_test.shape, Y_test.shape"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "((11286, 132), (11286,), (4838, 132), (4838,))"
      ]
     },
     "metadata": {},
     "execution_count": 25
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "source": [
    "m=RandomForestClassifier()\r\n",
    "m.fit(X_train, Y_train)\r\n",
    "predict_y = m.predict(X_test)\r\n",
    "print('ROCAUC score:',roc_auc_score(Y_test, predict_y))\r\n",
    "print('Accuracy score:',accuracy_score(Y_test, predict_y))\r\n",
    "print('F1 score:',f1_score(Y_test, predict_y))\r\n",
    "print(classification_report(Y_test, predict_y))\r\n",
    "confusion_matrix(Y_test, predict_y)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "ROCAUC score: 0.6153846153846154\n",
      "Accuracy score: 0.9979330301777594\n",
      "F1 score: 0.375\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      4825\n",
      "           1       1.00      0.23      0.38        13\n",
      "\n",
      "    accuracy                           1.00      4838\n",
      "   macro avg       1.00      0.62      0.69      4838\n",
      "weighted avg       1.00      1.00      1.00      4838\n",
      "\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([[4825,    0],\n",
       "       [  10,    3]], dtype=int64)"
      ]
     },
     "metadata": {},
     "execution_count": 26
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "source": [
    "\r\n",
    "\r\n",
    "smote = ADASYN()\r\n",
    "\r\n",
    "# fit target and predictor variable\r\n",
    "x_smote , y_smote = smote.fit_sample(X_train, Y_train)\r\n",
    "x_train1, y_train1 = x_smote , y_smote"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "source": [
    "#scaler = StandardScaler()  \r\n",
    "\r\n",
    "#scaler.fit(x_train1)  \r\n",
    "#x_train1 = scaler.transform(x_train1)  \r\n",
    "\r\n",
    "#x_test1 = scaler.transform(X_test)\r\n",
    "x_test1 =X_test\r\n",
    "\r\n",
    "#m = MLPClassifier(hidden_layer_sizes=(132,132))\r\n",
    "m=LogisticRegression()\r\n",
    "m.fit(x_train1, y_train1)\r\n",
    "predict_y = m.predict(x_test1)\r\n",
    "\r\n",
    "print('ROCAUC score:',roc_auc_score(Y_test, predict_y))\r\n",
    "print('Accuracy score:',accuracy_score(Y_test, predict_y))\r\n",
    "print('F1 score:',f1_score(Y_test, predict_y))\r\n",
    "print(classification_report(Y_test, predict_y))\r\n",
    "confusion_matrix(Y_test, predict_y)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "ROCAUC score: 0.776221602231965\n",
      "Accuracy score: 0.7831748656469616\n",
      "F1 score: 0.018709073900841908\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.78      0.88      4825\n",
      "           1       0.01      0.77      0.02        13\n",
      "\n",
      "    accuracy                           0.78      4838\n",
      "   macro avg       0.50      0.78      0.45      4838\n",
      "weighted avg       1.00      0.78      0.88      4838\n",
      "\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "C:\\Users\\zys\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([[3779, 1046],\n",
       "       [   3,   10]], dtype=int64)"
      ]
     },
     "metadata": {},
     "execution_count": 29
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "source": [
    "m=RandomForestClassifier()\r\n",
    "m.fit(x_train1, y_train1)\r\n",
    "predict_y = m.predict(x_test1)\r\n",
    "print('ROCAUC score:',roc_auc_score(Y_test, predict_y))\r\n",
    "print('Accuracy score:',accuracy_score(Y_test, predict_y))\r\n",
    "print('F1 score:',f1_score(Y_test, predict_y))\r\n",
    "print(classification_report(Y_test, predict_y))\r\n",
    "confusion_matrix(Y_test, predict_y)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "ROCAUC score: 0.6153846153846154\n",
      "Accuracy score: 0.9979330301777594\n",
      "F1 score: 0.375\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      4825\n",
      "           1       1.00      0.23      0.38        13\n",
      "\n",
      "    accuracy                           1.00      4838\n",
      "   macro avg       1.00      0.62      0.69      4838\n",
      "weighted avg       1.00      1.00      1.00      4838\n",
      "\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([[4825,    0],\n",
       "       [  10,    3]], dtype=int64)"
      ]
     },
     "metadata": {},
     "execution_count": 30
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(dataset_[:,:-1],\r\n",
    "                                                    dataset_[:,-1],\r\n",
    "                                                    test_size=0.3,\r\n",
    "                                                    stratify=dataset_[:,-1]\r\n",
    "                                                    )\r\n",
    "\r\n",
    "\r\n",
    "pipeline = imbpipeline(steps = [['smote', SMOTE()],\r\n",
    "                                \r\n",
    "                                ['classifier', RandomForestClassifier()]])\r\n",
    "\r\n",
    "stratified_kfold = StratifiedKFold(n_splits=5,\r\n",
    "                                       shuffle=True\r\n",
    "                                       )\r\n",
    "    \r\n",
    "param_grid = {'classifier__n_estimators':[10,20,30,40,50,60,70,80,90,100,150,200,300,400,500]}\r\n",
    "grid_search = GridSearchCV(estimator=pipeline,\r\n",
    "                           param_grid=param_grid,\r\n",
    "                           scoring='f1_macro',\r\n",
    "                           cv=stratified_kfold,\r\n",
    "                           n_jobs=-1)\r\n",
    "\r\n",
    "grid_search.fit(X_train, y_train)\r\n",
    "cv_score_RF_s = grid_search.best_score_\r\n",
    "test_score_RF_s = grid_search.score(X_test, y_test)\r\n",
    "print(f'Cross-validation score: {cv_score_RF_s}\\nTest score: {test_score_RF_s}')"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Cross-validation score: 0.7364387707699691\n",
      "Test score: 0.7217045166931272\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "source": [
    "pipeline = imbpipeline(steps = [['ADASYN', ADASYN()],\r\n",
    "                                \r\n",
    "                                ['classifier', RandomForestClassifier()]])\r\n",
    "\r\n",
    "stratified_kfold = StratifiedKFold(n_splits=5,\r\n",
    "                                       shuffle=True\r\n",
    "                                       )\r\n",
    "    \r\n",
    "param_grid = {'classifier__n_estimators':[10,20,30,40,50,60,70,80,90,100,150,200,300,400,500]}\r\n",
    "grid_search = GridSearchCV(estimator=pipeline,\r\n",
    "                           param_grid=param_grid,\r\n",
    "                           scoring='f1_macro',\r\n",
    "                           cv=stratified_kfold,\r\n",
    "                           n_jobs=-1)\r\n",
    "\r\n",
    "grid_search.fit(X_train, y_train)\r\n",
    "cv_score_RF_a = grid_search.best_score_\r\n",
    "test_score_RF_a = grid_search.score(X_test, y_test)\r\n",
    "print(f'Cross-validation score: {cv_score_RF_a}\\nTest score: {test_score_RF_a}')"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Cross-validation score: 0.6380167555411151\n",
      "Test score: 0.7217045166931272\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "source": [
    "pipeline = imbpipeline(steps = [['SVMSMOTE', SVMSMOTE()],\r\n",
    "                                \r\n",
    "                                ['classifier', RandomForestClassifier()]])\r\n",
    "\r\n",
    "stratified_kfold = StratifiedKFold(n_splits=5,\r\n",
    "                                       shuffle=True\r\n",
    "                                       )\r\n",
    "    \r\n",
    "param_grid = {'classifier__n_estimators':[10,20,30,40,50,60,70,80,90,100,150,200,300,400,500]}\r\n",
    "grid_search = GridSearchCV(estimator=pipeline,\r\n",
    "                           param_grid=param_grid,\r\n",
    "                           scoring='f1_macro',\r\n",
    "                           cv=stratified_kfold,\r\n",
    "                           n_jobs=-1)\r\n",
    "\r\n",
    "grid_search.fit(X_train, y_train)\r\n",
    "cv_score_RF_svm = grid_search.best_score_\r\n",
    "test_score_RF_svm = grid_search.score(X_test, y_test)\r\n",
    "print(f'Cross-validation score: {cv_score_RF_svm}\\nTest score: {test_score_RF_svm}')"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Cross-validation score: 0.7384006509729433\n",
      "Test score: 0.7217045166931272\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "source": [
    "pipeline = imbpipeline(steps = [['SMOTE', SMOTE()],\r\n",
    "                                \r\n",
    "                                ['classifier', LogisticRegression()]])\r\n",
    "    \r\n",
    "param_grid = {'classifier__C':[0.001, 0.01, 0.1, 1, 10, 100, 1000]}\r\n",
    "grid_search = GridSearchCV(estimator=pipeline,\r\n",
    "                           param_grid=param_grid,\r\n",
    "                           scoring='f1_macro',\r\n",
    "                           cv=stratified_kfold,\r\n",
    "                           n_jobs=-1)\r\n",
    "\r\n",
    "grid_search.fit(X_train, y_train)\r\n",
    "cv_score_LR_s = grid_search.best_score_\r\n",
    "test_score_LR_s= grid_search.score(X_test, y_test)\r\n",
    "print(f'Cross-validation score: {cv_score_LR_s}\\nTest score: {test_score_LR_s}')"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Cross-validation score: 0.46859527119273103\n",
      "Test score: 0.460230217236968\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "C:\\Users\\zys\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "source": [
    "pipeline = imbpipeline(steps = [['ADASYN', ADASYN()],\r\n",
    "                                \r\n",
    "                                ['classifier', LogisticRegression()]])\r\n",
    "    \r\n",
    "param_grid = {'classifier__C':[0.001, 0.01, 0.1, 1, 10, 100, 1000]}\r\n",
    "grid_search = GridSearchCV(estimator=pipeline,\r\n",
    "                           param_grid=param_grid,\r\n",
    "                           scoring='f1_macro',\r\n",
    "                           cv=stratified_kfold,\r\n",
    "                           n_jobs=-1)\r\n",
    "\r\n",
    "grid_search.fit(X_train, y_train)\r\n",
    "cv_score_LR_a = grid_search.best_score_\r\n",
    "test_score_LR_a= grid_search.score(X_test, y_test)\r\n",
    "print(f'Cross-validation score: {cv_score_LR_a}\\nTest score: {test_score_LR_a}')"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Cross-validation score: 0.4660421442959735\n",
      "Test score: 0.46159272672400786\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "C:\\Users\\zys\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "source": [
    "pipeline = imbpipeline(steps = [['SVMSMOTE', SVMSMOTE()],\r\n",
    "                                \r\n",
    "                                ['classifier', LogisticRegression()]])\r\n",
    "    \r\n",
    "param_grid = {'classifier__C':[0.001, 0.01, 0.1, 1, 10, 100, 1000]}\r\n",
    "grid_search = GridSearchCV(estimator=pipeline,\r\n",
    "                           param_grid=param_grid,\r\n",
    "                           scoring='f1_macro',\r\n",
    "                           cv=stratified_kfold,\r\n",
    "                           n_jobs=-1)\r\n",
    "\r\n",
    "grid_search.fit(X_train, y_train)\r\n",
    "cv_score_LR_svm = grid_search.best_score_\r\n",
    "test_score_LR_svm= grid_search.score(X_test, y_test)\r\n",
    "print(f'Cross-validation score: {cv_score_LR_svm}\\nTest score: {test_score_LR_svm}')"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Cross-validation score: 0.505768566428954\n",
      "Test score: 0.49551122647667695\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "C:\\Users\\zys\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "source": [
    "pipeline = imbpipeline(steps = [['SMOTE', SMOTE()],\r\n",
    "                                \r\n",
    "                                ['classifier', DecisionTreeClassifier()]])\r\n",
    "    \r\n",
    "param_grid = {'classifier__criterion':['gini','entropy'],'classifier__max_depth':[4,5,6,7,8,9,10,11,12,15,20,30,40,50,70,90,120,150,200,300,400,500]}\r\n",
    "grid_search = GridSearchCV(estimator=pipeline,\r\n",
    "                           param_grid=param_grid,\r\n",
    "                           scoring='f1_macro',\r\n",
    "                           cv=stratified_kfold,\r\n",
    "                           n_jobs=-1)\r\n",
    "\r\n",
    "grid_search.fit(X_train, y_train)\r\n",
    "cv_score_DT_s = grid_search.best_score_\r\n",
    "test_score_DT_s= grid_search.score(X_test, y_test)\r\n",
    "print(f'Cross-validation score: {cv_score_DT_s}\\nTest score: {test_score_DT_s}')"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Cross-validation score: 0.6459891368236471\n",
      "Test score: 0.6096588289534348\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "source": [
    "pipeline = imbpipeline(steps = [['ADASYN', ADASYN()],\r\n",
    "                                \r\n",
    "                                ['classifier', DecisionTreeClassifier()]])\r\n",
    "    \r\n",
    "param_grid = {'classifier__criterion':['gini','entropy'],'classifier__max_depth':[4,5,6,7,8,9,10,11,12,15,20,30,40,50,70,90,120,150,200,300,400,500]}\r\n",
    "grid_search = GridSearchCV(estimator=pipeline,\r\n",
    "                           param_grid=param_grid,\r\n",
    "                           scoring='f1_macro',\r\n",
    "                           cv=stratified_kfold,\r\n",
    "                           n_jobs=-1)\r\n",
    "\r\n",
    "grid_search.fit(X_train, y_train)\r\n",
    "cv_score_DT_a = grid_search.best_score_\r\n",
    "test_score_DT_a= grid_search.score(X_test, y_test)\r\n",
    "print(f'Cross-validation score: {cv_score_DT_a}\\nTest score: {test_score_DT_a}')"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Cross-validation score: 0.6155557627730616\n",
      "Test score: 0.606603802682234\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "source": [
    "pipeline = imbpipeline(steps = [['SVMSMOTE',SVMSMOTE()],\r\n",
    "                                \r\n",
    "                                ['classifier', DecisionTreeClassifier()]])\r\n",
    "    \r\n",
    "param_grid = {'classifier__criterion':['gini','entropy'],'classifier__max_depth':[4,5,6,7,8,9,10,11,12,15,20,30,40,50,70,90,120,150,200,300,400,500]}\r\n",
    "grid_search = GridSearchCV(estimator=pipeline,\r\n",
    "                           param_grid=param_grid,\r\n",
    "                           scoring='f1_macro',\r\n",
    "                           cv=stratified_kfold,\r\n",
    "                           n_jobs=-1)\r\n",
    "\r\n",
    "grid_search.fit(X_train, y_train)\r\n",
    "cv_score_DT_svm = grid_search.best_score_\r\n",
    "test_score_DT_svm= grid_search.score(X_test, y_test)\r\n",
    "print(f'Cross-validation score: {cv_score_DT_svm}\\nTest score: {test_score_DT_svm}')"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Cross-validation score: 0.6485632450025445\n",
      "Test score: 0.6418206586117035\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "source": [
    "pipeline = imbpipeline(steps = [['SMOTE', SMOTE()],\r\n",
    "                                \r\n",
    "                                ['classifier', KNeighborsClassifier()]])\r\n",
    "    \r\n",
    "param_grid = {'classifier__n_neighbors':[1,2,3,4,5,6,7,8,9,10,20,30,40,50,80,100,200,300,400,500,600,700,800,900,1000]}\r\n",
    "grid_search = GridSearchCV(estimator=pipeline,\r\n",
    "                           param_grid=param_grid,\r\n",
    "                           scoring='f1_macro',\r\n",
    "                           cv=stratified_kfold,\r\n",
    "                           n_jobs=-1)\r\n",
    "\r\n",
    "grid_search.fit(X_train, y_train)\r\n",
    "cv_score_knn_s = grid_search.best_score_\r\n",
    "test_score_knn_s= grid_search.score(X_test, y_test)\r\n",
    "print(f'Cross-validation score: {cv_score_knn_s}\\nTest score: {test_score_knn_s}')"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Cross-validation score: 0.5518849060766442\n",
      "Test score: 0.5702846991902184\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "source": [
    "pipeline = imbpipeline(steps = [['ADASYN', ADASYN()],\r\n",
    "                                \r\n",
    "                                ['classifier', KNeighborsClassifier()]])\r\n",
    "    \r\n",
    "param_grid = {'classifier__n_neighbors':[1,2,3,4,5,6,7,8,9,10,20,30,40,50,80,100,200,300,400,500,600,700,800,900,1000]}\r\n",
    "grid_search = GridSearchCV(estimator=pipeline,\r\n",
    "                           param_grid=param_grid,\r\n",
    "                           scoring='f1_macro',\r\n",
    "                           cv=stratified_kfold,\r\n",
    "                           n_jobs=-1)\r\n",
    "\r\n",
    "grid_search.fit(X_train, y_train)\r\n",
    "cv_score_knn_a = grid_search.best_score_\r\n",
    "test_score_knn_a= grid_search.score(X_test, y_test)\r\n",
    "print(f'Cross-validation score: {cv_score_knn_a}\\nTest score: {test_score_knn_a}')"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Cross-validation score: 0.5469521719987871\n",
      "Test score: 0.5689337689337689\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "source": [
    "pipeline = imbpipeline(steps = [['SVMSMOTE', SVMSMOTE()],\r\n",
    "                                \r\n",
    "                                ['classifier', KNeighborsClassifier()]])\r\n",
    "    \r\n",
    "param_grid = {'classifier__n_neighbors':[1,2,3,4,5,6,7,8,9,10,20,30,40,50,80,100,200,300,400,500,600,700,800,900,1000]}\r\n",
    "grid_search = GridSearchCV(estimator=pipeline,\r\n",
    "                           param_grid=param_grid,\r\n",
    "                           scoring='f1_macro',\r\n",
    "                           cv=stratified_kfold,\r\n",
    "                           n_jobs=-1)\r\n",
    "\r\n",
    "grid_search.fit(X_train, y_train)\r\n",
    "cv_score_knn_svm = grid_search.best_score_\r\n",
    "test_score_knn_svm= grid_search.score(X_test, y_test)\r\n",
    "print(f'Cross-validation score: {cv_score_knn_svm}\\nTest score: {test_score_knn_svm}')"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Cross-validation score: 0.6210220041564852\n",
      "Test score: 0.61991584411993\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "source": [
    "pipeline = imbpipeline(steps = [['SMOTE', SMOTE()],\r\n",
    "                                \r\n",
    "                                ['classifier', XGBClassifier()]])\r\n",
    "    \r\n",
    "param_grid = {'classifier__min_child_weight': [1, 5, 10],\r\n",
    "        'classifier__max_depth': [3, 4, 5]}\r\n",
    "grid_search = GridSearchCV(estimator=pipeline,\r\n",
    "                           param_grid=param_grid,\r\n",
    "                           scoring='f1_macro',\r\n",
    "                           cv=stratified_kfold,\r\n",
    "                           n_jobs=-1)\r\n",
    "\r\n",
    "grid_search.fit(X_train, y_train)\r\n",
    "cv_score_xgb_s = grid_search.best_score_\r\n",
    "test_score_xgb_s= grid_search.score(X_test, y_test)\r\n",
    "print(f'Cross-validation score: {cv_score_xgb_s}\\nTest score: {test_score_xgb_s}')"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "C:\\Users\\zys\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[07:15:14] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "Cross-validation score: 0.7297498671745943\n",
      "Test score: 0.7217045166931272\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "source": [
    "pipeline = imbpipeline(steps = [['ADASYN', ADASYN()],\r\n",
    "                                \r\n",
    "                                ['classifier', XGBClassifier()]])\r\n",
    "\r\n",
    "grid_search = GridSearchCV(estimator=pipeline,\r\n",
    "                           param_grid=param_grid,\r\n",
    "                           scoring='f1_macro',\r\n",
    "                           cv=stratified_kfold,\r\n",
    "                           n_jobs=-1)\r\n",
    "\r\n",
    "grid_search.fit(X_train, y_train)\r\n",
    "cv_score_xgb_a = grid_search.best_score_\r\n",
    "test_score_xgb_a= grid_search.score(X_test, y_test)\r\n",
    "print(f'Cross-validation score: {cv_score_xgb_a}\\nTest score: {test_score_xgb_a}')"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "C:\\Users\\zys\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[07:16:25] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "Cross-validation score: 0.7197054226777085\n",
      "Test score: 0.7217045166931272\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "source": [
    "pipeline = imbpipeline(steps = [['SVMSMOTE', SVMSMOTE()],\r\n",
    "                                \r\n",
    "                                ['classifier', XGBClassifier()]])\r\n",
    "    \r\n",
    "grid_search = GridSearchCV(estimator=pipeline,\r\n",
    "                           param_grid=param_grid,\r\n",
    "                           scoring='f1_macro',\r\n",
    "                           cv=stratified_kfold,\r\n",
    "                           n_jobs=-1)\r\n",
    "\r\n",
    "grid_search.fit(X_train, y_train)\r\n",
    "cv_score_xgb_svm = grid_search.best_score_\r\n",
    "test_score_xgb_svm= grid_search.score(X_test, y_test)\r\n",
    "print(f'Cross-validation score: {cv_score_xgb_svm}\\nTest score: {test_score_xgb_svm}')"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "C:\\Users\\zys\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[07:17:05] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "Cross-validation score: 0.688356344484698\n",
      "Test score: 0.7217045166931272\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "source": [
    "models = pd.DataFrame({\r\n",
    "    'Model': ['Random Forest', 'Logistic Regression', \r\n",
    "              'Decision Tree','KNN',  \r\n",
    "              'XGBoost'],\r\n",
    "    'SMOTE': [cv_score_RF_s,cv_score_LR_s,cv_score_DT_s,cv_score_knn_s,cv_score_xgb_s],\r\n",
    "    'ADASYN': [cv_score_RF_a,cv_score_LR_a,cv_score_DT_a,cv_score_knn_a,cv_score_xgb_a],\r\n",
    "    'SVMSMOTE':[cv_score_RF_svm,cv_score_LR_svm,cv_score_DT_svm,cv_score_knn_svm,cv_score_xgb_svm]})"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "source": [
    "models"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "                 Model     SMOTE    ADASYN  SVMSMOTE\n",
       "0        Random Forest  0.736439  0.638017  0.738401\n",
       "1  Logistic Regression  0.468595  0.466042  0.505769\n",
       "2        Decision Tree  0.645989  0.615556  0.648563\n",
       "3                  KNN  0.551885  0.546952  0.621022\n",
       "4              XGBoost  0.729750  0.719705  0.688356"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>SMOTE</th>\n",
       "      <th>ADASYN</th>\n",
       "      <th>SVMSMOTE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Random Forest</td>\n",
       "      <td>0.736439</td>\n",
       "      <td>0.638017</td>\n",
       "      <td>0.738401</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>0.468595</td>\n",
       "      <td>0.466042</td>\n",
       "      <td>0.505769</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Decision Tree</td>\n",
       "      <td>0.645989</td>\n",
       "      <td>0.615556</td>\n",
       "      <td>0.648563</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>KNN</td>\n",
       "      <td>0.551885</td>\n",
       "      <td>0.546952</td>\n",
       "      <td>0.621022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>XGBoost</td>\n",
       "      <td>0.729750</td>\n",
       "      <td>0.719705</td>\n",
       "      <td>0.688356</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "metadata": {},
     "execution_count": 59
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# To do:\r\n",
    "1. add features\r\n",
    "2. more algorithms\r\n",
    "3. bayesian optimisation\r\n",
    "4. oversampling strategy?"
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}